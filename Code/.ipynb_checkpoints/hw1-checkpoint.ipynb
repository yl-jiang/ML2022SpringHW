{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['covid.test.csv', 'covid.train.csv']\n"
     ]
    }
   ],
   "source": [
    "zipf = zipfile.ZipFile(\"../Dataset/ml2022spring-hw1.zip\")\n",
    "namelist = zipf.namelist()\n",
    "print(namelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipf.extractall('../Dataset/hw1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"../Dataset/hw1/covid.train.csv\"\n",
    "test_data_path = \"../Dataset/hw1/covid.test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_data_path)\n",
    "test_csv = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AL</th>\n",
       "      <th>AK</th>\n",
       "      <th>AZ</th>\n",
       "      <th>AR</th>\n",
       "      <th>CA</th>\n",
       "      <th>CO</th>\n",
       "      <th>CT</th>\n",
       "      <th>FL</th>\n",
       "      <th>GA</th>\n",
       "      <th>...</th>\n",
       "      <th>work_outside_home.4</th>\n",
       "      <th>shop.4</th>\n",
       "      <th>restaurant.4</th>\n",
       "      <th>spent_time.4</th>\n",
       "      <th>large_event.4</th>\n",
       "      <th>public_transit.4</th>\n",
       "      <th>anxious.4</th>\n",
       "      <th>depressed.4</th>\n",
       "      <th>worried_finances.4</th>\n",
       "      <th>tested_positive.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.113209</td>\n",
       "      <td>67.394551</td>\n",
       "      <td>36.674291</td>\n",
       "      <td>40.743132</td>\n",
       "      <td>17.842221</td>\n",
       "      <td>4.093712</td>\n",
       "      <td>10.440071</td>\n",
       "      <td>8.627117</td>\n",
       "      <td>37.329512</td>\n",
       "      <td>7.456154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.920257</td>\n",
       "      <td>64.398380</td>\n",
       "      <td>34.612238</td>\n",
       "      <td>44.035688</td>\n",
       "      <td>17.808103</td>\n",
       "      <td>4.924935</td>\n",
       "      <td>10.172662</td>\n",
       "      <td>9.954333</td>\n",
       "      <td>32.508881</td>\n",
       "      <td>8.010957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.604604</td>\n",
       "      <td>62.101064</td>\n",
       "      <td>26.521875</td>\n",
       "      <td>36.746453</td>\n",
       "      <td>13.903667</td>\n",
       "      <td>7.313833</td>\n",
       "      <td>10.388712</td>\n",
       "      <td>7.956139</td>\n",
       "      <td>36.745588</td>\n",
       "      <td>2.906977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.115738</td>\n",
       "      <td>67.935520</td>\n",
       "      <td>38.022492</td>\n",
       "      <td>48.434809</td>\n",
       "      <td>27.134876</td>\n",
       "      <td>3.101904</td>\n",
       "      <td>10.498683</td>\n",
       "      <td>8.231522</td>\n",
       "      <td>38.680162</td>\n",
       "      <td>12.575816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.129714</td>\n",
       "      <td>69.934592</td>\n",
       "      <td>38.242368</td>\n",
       "      <td>49.095933</td>\n",
       "      <td>22.683709</td>\n",
       "      <td>4.594620</td>\n",
       "      <td>9.878927</td>\n",
       "      <td>9.469290</td>\n",
       "      <td>28.344123</td>\n",
       "      <td>21.428589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  AL  AK  AZ  AR  CA  CO  CT  FL  GA  ...  work_outside_home.4  \\\n",
       "0   0   0   0   0   0   0   0   0   1   0  ...            31.113209   \n",
       "1   1   0   0   0   0   0   1   0   0   0  ...            33.920257   \n",
       "2   2   0   0   0   0   0   0   0   0   0  ...            31.604604   \n",
       "3   3   0   0   0   0   0   0   0   0   0  ...            35.115738   \n",
       "4   4   0   0   0   0   0   0   0   0   0  ...            35.129714   \n",
       "\n",
       "      shop.4  restaurant.4  spent_time.4  large_event.4  public_transit.4  \\\n",
       "0  67.394551     36.674291     40.743132      17.842221          4.093712   \n",
       "1  64.398380     34.612238     44.035688      17.808103          4.924935   \n",
       "2  62.101064     26.521875     36.746453      13.903667          7.313833   \n",
       "3  67.935520     38.022492     48.434809      27.134876          3.101904   \n",
       "4  69.934592     38.242368     49.095933      22.683709          4.594620   \n",
       "\n",
       "   anxious.4  depressed.4  worried_finances.4  tested_positive.4  \n",
       "0  10.440071     8.627117           37.329512           7.456154  \n",
       "1  10.172662     9.954333           32.508881           8.010957  \n",
       "2  10.388712     7.956139           36.745588           2.906977  \n",
       "3  10.498683     8.231522           38.680162          12.575816  \n",
       "4   9.878927     9.469290           28.344123          21.428589  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共118列：\n",
    "+ 0：id\n",
    "+ 1~37: State\n",
    "+ 38~41: COVID-like illness  (5天)\n",
    "+ 42~49: Behavior Indicators  (5天)\n",
    "+ 50~52: Medical Health Indicators  (5天)\n",
    "+ 53: Tested Positive Cases  (5天)\n",
    "\n",
    "$$1 + 37 + 3\\times 5 + 8\\times 5 + 3\\times 5 + 1\\times 5 = 118$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "+ 统计信息\n",
    "+ 缺失值\n",
    "+ 特征相关性\n",
    "+ 特征规约"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉'id'列\n",
    "coulmns = train_csv.columns\n",
    "train_dataset = train_csv[coulmns[1:]]\n",
    "\n",
    "coulmns = test_csv.columns\n",
    "test_dataset = test_csv[coulmns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan元素总数：0\n",
      "AL                    0\n",
      "AK                    0\n",
      "AZ                    0\n",
      "AR                    0\n",
      "CA                    0\n",
      "                     ..\n",
      "public_transit.4      0\n",
      "anxious.4             0\n",
      "depressed.4           0\n",
      "worried_finances.4    0\n",
      "tested_positive.4     0\n",
      "Length: 117, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 查看每一列Nan值的个数\n",
    "nacount = train_dataset.isna().sum()\n",
    "print(f\"Nan元素总数：{nacount.sum()}\")\n",
    "\n",
    "print(nacount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_pred(preds, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "**All original features, linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val(model, x, y):\n",
    "    scores = cross_val_score(model, x, y, scoring='neg_mean_squared_error', cv=10)\n",
    "    print(np.mean(np.sqrt(-scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0519071309202654\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "# 使用全部原始特征\n",
    "cross_val(lin_reg, train_dataset.iloc[:, :-1], train_dataset.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_dataset.iloc[:, :-1], train_dataset.iloc[:, -1])\n",
    "preds = lin_reg.predict(test_dataset)\n",
    "\n",
    "save_pred(preds, './plain_line_reg.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All original features, decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.628028341143036\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "# 使用全部原始特征\n",
    "cross_val(tree_reg, train_dataset.iloc[:, :-1], train_dataset.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test results\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(train_dataset.iloc[:, :-1], train_dataset.iloc[:, -1])\n",
    "preds = tree_reg.predict(test_dataset)\n",
    "\n",
    "save_pred(preds, './plain_tree_reg.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相关性大于0.5的特征数：35\n"
     ]
    }
   ],
   "source": [
    "# 特征与特征之间的相关性矩阵\n",
    "corr_matrix = train_dataset.corr()\n",
    "\n",
    "# 查看与test_positive.4与其他特征之间的相关性\n",
    "positive4_coor = corr_matrix['tested_positive.4'].sort_values(ascending=False)\n",
    "mask = positive4_coor > 0.5\n",
    "print(f'相关性大于0.5的特征数：{sum(mask)}')\n",
    "# print(positive4_coor[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Original features, linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.049216997777881\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "# 使用相关性较大的若干原始特征\n",
    "selcted_columns = list(positive4_coor.index[mask])\n",
    "cross_val(lin_reg, train_dataset[selcted_columns[1:]], train_dataset[selcted_columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_dataset[selcted_columns[1:]], train_dataset[selcted_columns[0]])\n",
    "preds = lin_reg.predict(test_dataset[selcted_columns[1:]])\n",
    "\n",
    "save_pred(preds, './plain_line_reg_with_feature_selction.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Original features, Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5763473098744598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "# 使用相关性较大的若干原始特征\n",
    "selcted_columns = list(positive4_coor.index[mask])\n",
    "cross_val(tree_reg, train_dataset[selcted_columns[1:]], train_dataset[selcted_columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "tree_reg = LinearRegression()\n",
    "tree_reg.fit(train_dataset[selcted_columns[1:]], train_dataset[selcted_columns[0]])\n",
    "preds = tree_reg.predict(test_dataset[selcted_columns[1:]])\n",
    "\n",
    "save_pred(preds, './plain_tree_reg_with_feature_selction.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, skip=None):\n",
    "        self.skip = skip\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for col in x.columns:\n",
    "            if self.skip not in col and x[col].max() > 1:\n",
    "                mean = x[col].mean()\n",
    "                std = x[col].std()\n",
    "                x[col] = x[col].map(lambda i: (i - mean) / std)\n",
    "                \n",
    "        return x\n",
    "\n",
    "\n",
    "class MaxminScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, skip=None):\n",
    "        self.skip = skip\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for col in x.columns:\n",
    "            if self.skip not in col and x[col].max() > 1:\n",
    "                max_v = x[col].max()\n",
    "                min_v = x[col].min()\n",
    "                x[col] = x[col].map(lambda i: (i - min_v) / (max_v - min_v))\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "normal_scaled_pipeline = Pipeline([('std_scaler', NormalScaler('tested_positive'))])\n",
    "train_dataset_normal_scaled = normal_scaled_pipeline.transform(train_dataset.copy())\n",
    "test_dataset_normal_scaled = normal_scaled_pipeline.transform(test_dataset.copy())\n",
    "\n",
    "maxmin_scaled_pipeline = Pipeline([('maxmin_scaler', MaxminScaler('tested_positive'))])\n",
    "train_dataset_maxmin_scaled = maxmin_scaled_pipeline.transform(train_dataset.copy())\n",
    "test_dataset_maxmin_scaled = maxmin_scaled_pipeline.transform(test_dataset.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AL</th>\n",
       "      <th>AK</th>\n",
       "      <th>AZ</th>\n",
       "      <th>AR</th>\n",
       "      <th>CA</th>\n",
       "      <th>CO</th>\n",
       "      <th>CT</th>\n",
       "      <th>FL</th>\n",
       "      <th>GA</th>\n",
       "      <th>ID</th>\n",
       "      <th>...</th>\n",
       "      <th>work_outside_home.4</th>\n",
       "      <th>shop.4</th>\n",
       "      <th>restaurant.4</th>\n",
       "      <th>spent_time.4</th>\n",
       "      <th>large_event.4</th>\n",
       "      <th>public_transit.4</th>\n",
       "      <th>anxious.4</th>\n",
       "      <th>depressed.4</th>\n",
       "      <th>worried_finances.4</th>\n",
       "      <th>tested_positive.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497704</td>\n",
       "      <td>0.375942</td>\n",
       "      <td>0.454340</td>\n",
       "      <td>-1.121859</td>\n",
       "      <td>-0.626113</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>-0.671085</td>\n",
       "      <td>-0.854542</td>\n",
       "      <td>0.691637</td>\n",
       "      <td>7.456154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235301</td>\n",
       "      <td>-0.845236</td>\n",
       "      <td>-0.052472</td>\n",
       "      <td>-0.221926</td>\n",
       "      <td>-0.635917</td>\n",
       "      <td>0.108189</td>\n",
       "      <td>-0.788870</td>\n",
       "      <td>-0.024633</td>\n",
       "      <td>-0.642577</td>\n",
       "      <td>8.010957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369386</td>\n",
       "      <td>-1.781575</td>\n",
       "      <td>-2.040928</td>\n",
       "      <td>-2.214246</td>\n",
       "      <td>-1.757860</td>\n",
       "      <td>1.229533</td>\n",
       "      <td>-0.693707</td>\n",
       "      <td>-1.274106</td>\n",
       "      <td>0.530023</td>\n",
       "      <td>2.906977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547478</td>\n",
       "      <td>0.596431</td>\n",
       "      <td>0.785702</td>\n",
       "      <td>0.980457</td>\n",
       "      <td>2.044140</td>\n",
       "      <td>-0.747538</td>\n",
       "      <td>-0.645268</td>\n",
       "      <td>-1.101909</td>\n",
       "      <td>1.065458</td>\n",
       "      <td>12.575816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551127</td>\n",
       "      <td>1.411212</td>\n",
       "      <td>0.839743</td>\n",
       "      <td>1.161158</td>\n",
       "      <td>0.765093</td>\n",
       "      <td>-0.046860</td>\n",
       "      <td>-0.918250</td>\n",
       "      <td>-0.327931</td>\n",
       "      <td>-1.795264</td>\n",
       "      <td>21.428589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AL  AK  AZ  AR  CA  CO  CT  FL  GA  ID  ...  work_outside_home.4    shop.4  \\\n",
       "0   0   0   0   0   0   0   0   1   0   0  ...            -0.497704  0.375942   \n",
       "1   0   0   0   0   0   1   0   0   0   0  ...             0.235301 -0.845236   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...            -0.369386 -1.781575   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...             0.547478  0.596431   \n",
       "4   0   0   0   0   0   0   0   0   0   1  ...             0.551127  1.411212   \n",
       "\n",
       "   restaurant.4  spent_time.4  large_event.4  public_transit.4  anxious.4  \\\n",
       "0      0.454340     -1.121859      -0.626113         -0.281985  -0.671085   \n",
       "1     -0.052472     -0.221926      -0.635917          0.108189  -0.788870   \n",
       "2     -2.040928     -2.214246      -1.757860          1.229533  -0.693707   \n",
       "3      0.785702      0.980457       2.044140         -0.747538  -0.645268   \n",
       "4      0.839743      1.161158       0.765093         -0.046860  -0.918250   \n",
       "\n",
       "   depressed.4  worried_finances.4  tested_positive.4  \n",
       "0    -0.854542            0.691637           7.456154  \n",
       "1    -0.024633           -0.642577           8.010957  \n",
       "2    -1.274106            0.530023           2.906977  \n",
       "3    -1.101909            1.065458          12.575816  \n",
       "4    -0.327931           -1.795264          21.428589  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_normal_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AL</th>\n",
       "      <th>AK</th>\n",
       "      <th>AZ</th>\n",
       "      <th>AR</th>\n",
       "      <th>CA</th>\n",
       "      <th>CO</th>\n",
       "      <th>CT</th>\n",
       "      <th>FL</th>\n",
       "      <th>GA</th>\n",
       "      <th>ID</th>\n",
       "      <th>...</th>\n",
       "      <th>work_outside_home.4</th>\n",
       "      <th>shop.4</th>\n",
       "      <th>restaurant.4</th>\n",
       "      <th>spent_time.4</th>\n",
       "      <th>large_event.4</th>\n",
       "      <th>public_transit.4</th>\n",
       "      <th>anxious.4</th>\n",
       "      <th>depressed.4</th>\n",
       "      <th>worried_finances.4</th>\n",
       "      <th>tested_positive.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327454</td>\n",
       "      <td>0.622730</td>\n",
       "      <td>0.654060</td>\n",
       "      <td>0.297424</td>\n",
       "      <td>0.373811</td>\n",
       "      <td>0.184499</td>\n",
       "      <td>0.252049</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>0.608567</td>\n",
       "      <td>7.456154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467725</td>\n",
       "      <td>0.405868</td>\n",
       "      <td>0.560692</td>\n",
       "      <td>0.475805</td>\n",
       "      <td>0.372223</td>\n",
       "      <td>0.242983</td>\n",
       "      <td>0.230257</td>\n",
       "      <td>0.397139</td>\n",
       "      <td>0.378250</td>\n",
       "      <td>8.010957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352010</td>\n",
       "      <td>0.239588</td>\n",
       "      <td>0.194364</td>\n",
       "      <td>0.080897</td>\n",
       "      <td>0.190477</td>\n",
       "      <td>0.411063</td>\n",
       "      <td>0.247863</td>\n",
       "      <td>0.186614</td>\n",
       "      <td>0.580669</td>\n",
       "      <td>2.906977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527464</td>\n",
       "      <td>0.661885</td>\n",
       "      <td>0.715106</td>\n",
       "      <td>0.714135</td>\n",
       "      <td>0.806369</td>\n",
       "      <td>0.114716</td>\n",
       "      <td>0.256825</td>\n",
       "      <td>0.215628</td>\n",
       "      <td>0.673097</td>\n",
       "      <td>12.575816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528163</td>\n",
       "      <td>0.806577</td>\n",
       "      <td>0.725062</td>\n",
       "      <td>0.749953</td>\n",
       "      <td>0.599174</td>\n",
       "      <td>0.219742</td>\n",
       "      <td>0.206319</td>\n",
       "      <td>0.346036</td>\n",
       "      <td>0.179269</td>\n",
       "      <td>21.428589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AL  AK  AZ  AR  CA  CO  CT  FL  GA  ID  ...  work_outside_home.4    shop.4  \\\n",
       "0   0   0   0   0   0   0   0   1   0   0  ...             0.327454  0.622730   \n",
       "1   0   0   0   0   0   1   0   0   0   0  ...             0.467725  0.405868   \n",
       "2   0   0   0   0   0   0   0   0   0   0  ...             0.352010  0.239588   \n",
       "3   0   0   0   0   0   0   0   0   0   0  ...             0.527464  0.661885   \n",
       "4   0   0   0   0   0   0   0   0   0   1  ...             0.528163  0.806577   \n",
       "\n",
       "   restaurant.4  spent_time.4  large_event.4  public_transit.4  anxious.4  \\\n",
       "0      0.654060      0.297424       0.373811          0.184499   0.252049   \n",
       "1      0.560692      0.475805       0.372223          0.242983   0.230257   \n",
       "2      0.194364      0.080897       0.190477          0.411063   0.247863   \n",
       "3      0.715106      0.714135       0.806369          0.114716   0.256825   \n",
       "4      0.725062      0.749953       0.599174          0.219742   0.206319   \n",
       "\n",
       "   depressed.4  worried_finances.4  tested_positive.4  \n",
       "0     0.257307            0.608567           7.456154  \n",
       "1     0.397139            0.378250           8.010957  \n",
       "2     0.186614            0.580669           2.906977  \n",
       "3     0.215628            0.673097          12.575816  \n",
       "4     0.346036            0.179269          21.428589  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_maxmin_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0519556275693465\n",
      "1.051955627569344\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "cross_val(lin_reg, train_dataset_normal_scaled.iloc[:, :-1], train_dataset_normal_scaled.iloc[:, -1])\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "cross_val(lin_reg, train_dataset_maxmin_scaled.iloc[:, :-1], train_dataset_maxmin_scaled.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_dataset_normal_scaled.iloc[:, :-1], train_dataset_normal_scaled.iloc[:, -1])\n",
    "preds = lin_reg.predict(test_dataset_normal_scaled)\n",
    "\n",
    "save_pred(preds, './line_reg_with_normal_scaled_features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_dataset_maxmin_scaled.iloc[:, :-1], train_dataset_maxmin_scaled.iloc[:, -1])\n",
    "preds = lin_reg.predict(test_dataset_maxmin_scaled)\n",
    "\n",
    "save_pred(preds, './tree_reg_with_normal_scaled_features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimal score: 1.0468164063875016, index 65, thr: 0.6252525252525253\n"
     ]
    }
   ],
   "source": [
    "# 特征与特征之间的相关性矩阵\n",
    "corr_matrix = train_dataset_normal_scaled.corr()\n",
    "\n",
    "# 查看与test_positive.4与其他特征之间的相关性\n",
    "positive4_coor = corr_matrix['tested_positive.4'].sort_values(ascending=False)\n",
    "\n",
    "scores = []\n",
    "thrs = []\n",
    "for thr in np.linspace(0.1, 0.9, num=100):\n",
    "    mask = positive4_coor > thr\n",
    "    # print(f'相关性大于{thr}的特征数：{sum(mask)}')\n",
    "    thrs.append(thr)\n",
    "    lin_reg = LinearRegression()\n",
    "    # 使用相关性较大的若干原始特征\n",
    "    selcted_columns = list(positive4_coor.index[mask])\n",
    "    x, y = train_dataset_normal_scaled[selcted_columns[1:]], train_dataset_normal_scaled[selcted_columns[0]]\n",
    "    score_list = cross_val_score(lin_reg, x, y, scoring='neg_mean_squared_error', cv=10)\n",
    "    scores.append(np.mean(np.sqrt(-score_list)))\n",
    "\n",
    "print(f\"minimal score: {min(scores)}, index {np.argmin(scores)}, thr: {thrs[np.argmin(scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test result\n",
    "\n",
    "# 特征与特征之间的相关性矩阵\n",
    "corr_matrix = train_dataset_normal_scaled.corr()\n",
    "# 查看与test_positive.4与其他特征之间的相关性\n",
    "positive4_coor = corr_matrix['tested_positive.4'].sort_values(ascending=False)\n",
    "\n",
    "mask = positive4_coor > thrs[np.argmin(scores)]\n",
    "# 使用相关性较大的若干原始特征\n",
    "selcted_columns = list(positive4_coor.index[mask])\n",
    "x = train_dataset_normal_scaled[selcted_columns[1:]]\n",
    "y = train_dataset_normal_scaled[selcted_columns[0]]\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x, y)\n",
    "preds = lin_reg.predict(test_dataset_normal_scaled[selcted_columns[1:]])\n",
    "\n",
    "save_pred(preds, './line_reg_with_selected_normal_scaled_feature.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(dataset, valid_ratio, seed):\n",
    "    valid_set_size = int(valid_ratio * len(dataset))\n",
    "    train_set_size = len(dataset) - valid_set_size\n",
    "    train_set, valid_set = random_split(dataset, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "\n",
    "class COVID19Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y=None):\n",
    "        \n",
    "        \n",
    "        if y is not None:\n",
    "            shuffle_idx = np.random.permutation(len(x))\n",
    "            self.y = torch.from_numpy(y[shuffle_idx])\n",
    "            self.x = torch.from_numpy(x[shuffle_idx])\n",
    "        else:\n",
    "            self.y = y\n",
    "            self.x = torch.from_numpy(x)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.y is None:\n",
    "            return self.x[item]\n",
    "        else:\n",
    "            return self.x[item], self.y[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_channel, 32)\n",
    "        self.bn = torch.nn.BatchNorm1d(32)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "        self.linear2 = torch.nn.Linear(32, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features num: 35\n",
      "(2159, 34) (540, 34) (2159,) (540,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "same_seed(77)\n",
    "dataset = pd.read_csv(train_data_path)\n",
    "dataset = dataset[dataset.columns[1:]]  # remove 'id' column\n",
    "corr_matrix = dataset.corr()\n",
    "target_coor = corr_matrix['tested_positive.4'].sort_values(ascending=False)\n",
    "mask = target_coor > 0.5\n",
    "print(f\"selected features num: {np.sum(mask)}\")\n",
    "selected_feature_idx = list(target_coor.index[mask])\n",
    "\n",
    "x_dataset = dataset[selected_feature_idx[1:]]\n",
    "y_dataset = dataset.iloc[:, -1]\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=77)\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader, model, loss function, optimizer\n",
    "\n",
    "train_loader = DataLoader(COVID19Dataset(x_train.values, y_train.values), \n",
    "                          batch_size=16, \n",
    "                          shuffle=True, \n",
    "                          num_workers=0, \n",
    "                          drop_last=False)\n",
    "\n",
    "val_loader = DataLoader(COVID19Dataset(x_val.values, y_val.values), \n",
    "                        batch_size=8, \n",
    "                        shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Model(input_channel=len(selected_feature_idx[1:])).to(device=device)\n",
    "loss_fcn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/300: 100%|███| 135/135 [00:00<00:00, 599.62it/s, train loss 98.666 ; val loss 83.557; lr 1.00e-05]\n",
      "2/300: 100%|██| 135/135 [00:00<00:00, 653.86it/s, train loss 121.006 ; val loss 54.695; lr 1.00e-05]\n",
      "3/300: 100%|███| 135/135 [00:00<00:00, 655.91it/s, train loss 89.465 ; val loss 37.709; lr 1.00e-05]\n",
      "4/300: 100%|███| 135/135 [00:00<00:00, 649.88it/s, train loss 34.463 ; val loss 26.026; lr 1.00e-05]\n",
      "5/300: 100%|████| 135/135 [00:00<00:00, 645.43it/s, train loss 6.671 ; val loss 16.914; lr 1.00e-05]\n",
      "6/300: 100%|███| 135/135 [00:00<00:00, 665.16it/s, train loss 12.933 ; val loss 10.652; lr 1.00e-05]\n",
      "7/300: 100%|████| 135/135 [00:00<00:00, 668.10it/s, train loss 19.904 ; val loss 9.383; lr 1.00e-05]\n",
      "8/300: 100%|█████| 135/135 [00:00<00:00, 659.89it/s, train loss 8.457 ; val loss 7.764; lr 1.00e-05]\n",
      "9/300: 100%|████| 135/135 [00:00<00:00, 661.56it/s, train loss 18.707 ; val loss 5.032; lr 1.00e-05]\n",
      "10/300: 100%|███| 135/135 [00:00<00:00, 672.51it/s, train loss 11.446 ; val loss 5.111; lr 1.00e-05]\n",
      "11/300: 100%|████| 135/135 [00:00<00:00, 659.07it/s, train loss 6.043 ; val loss 4.294; lr 1.00e-05]\n",
      "12/300: 100%|███| 135/135 [00:00<00:00, 665.72it/s, train loss 11.317 ; val loss 4.896; lr 1.00e-05]\n",
      "13/300: 100%|████| 135/135 [00:00<00:00, 592.18it/s, train loss 9.822 ; val loss 4.866; lr 1.00e-05]\n",
      "14/300: 100%|████| 135/135 [00:00<00:00, 512.72it/s, train loss 4.279 ; val loss 7.652; lr 1.00e-05]\n",
      "15/300: 100%|████| 135/135 [00:00<00:00, 588.55it/s, train loss 6.266 ; val loss 3.945; lr 1.00e-05]\n",
      "16/300: 100%|████| 135/135 [00:00<00:00, 633.39it/s, train loss 6.446 ; val loss 5.675; lr 1.00e-05]\n",
      "17/300: 100%|███| 135/135 [00:00<00:00, 604.79it/s, train loss 16.832 ; val loss 4.280; lr 1.00e-05]\n",
      "18/300: 100%|███| 135/135 [00:00<00:00, 648.93it/s, train loss 12.640 ; val loss 4.828; lr 1.00e-05]\n",
      "19/300: 100%|███| 135/135 [00:00<00:00, 621.14it/s, train loss 23.822 ; val loss 3.987; lr 1.00e-05]\n",
      "20/300: 100%|████| 135/135 [00:00<00:00, 641.46it/s, train loss 5.558 ; val loss 3.639; lr 1.00e-05]\n",
      "21/300: 100%|████| 135/135 [00:00<00:00, 571.00it/s, train loss 8.783 ; val loss 3.977; lr 1.00e-05]\n",
      "22/300: 100%|████| 135/135 [00:00<00:00, 214.30it/s, train loss 6.618 ; val loss 6.891; lr 1.00e-05]\n",
      "23/300: 100%|████| 135/135 [00:00<00:00, 644.46it/s, train loss 5.902 ; val loss 4.100; lr 1.00e-05]\n",
      "24/300: 100%|███| 135/135 [00:00<00:00, 500.46it/s, train loss 14.854 ; val loss 3.389; lr 1.00e-05]\n",
      "25/300: 100%|████| 135/135 [00:00<00:00, 527.84it/s, train loss 7.684 ; val loss 3.934; lr 1.00e-05]\n",
      "26/300: 100%|███| 135/135 [00:00<00:00, 531.18it/s, train loss 11.887 ; val loss 3.737; lr 1.00e-05]\n",
      "27/300: 100%|████| 135/135 [00:00<00:00, 538.94it/s, train loss 2.922 ; val loss 5.869; lr 1.00e-05]\n",
      "28/300: 100%|████| 135/135 [00:00<00:00, 482.96it/s, train loss 5.180 ; val loss 4.186; lr 1.00e-05]\n",
      "29/300: 100%|████| 135/135 [00:00<00:00, 590.29it/s, train loss 5.677 ; val loss 3.712; lr 1.00e-05]\n",
      "30/300: 100%|███| 135/135 [00:00<00:00, 681.96it/s, train loss 15.149 ; val loss 4.629; lr 1.00e-05]\n",
      "31/300: 100%|███| 135/135 [00:00<00:00, 670.41it/s, train loss 15.534 ; val loss 3.373; lr 1.00e-05]\n",
      "32/300: 100%|███| 135/135 [00:00<00:00, 533.06it/s, train loss 10.471 ; val loss 3.380; lr 1.00e-05]\n",
      "33/300: 100%|████| 135/135 [00:00<00:00, 492.77it/s, train loss 6.217 ; val loss 3.361; lr 1.00e-05]\n",
      "34/300: 100%|███| 135/135 [00:00<00:00, 512.36it/s, train loss 11.688 ; val loss 3.918; lr 1.00e-05]\n",
      "35/300: 100%|███| 135/135 [00:00<00:00, 497.51it/s, train loss 37.686 ; val loss 3.509; lr 1.00e-05]\n",
      "36/300: 100%|████| 135/135 [00:00<00:00, 413.19it/s, train loss 3.965 ; val loss 3.107; lr 1.00e-05]\n",
      "37/300: 100%|████| 135/135 [00:00<00:00, 517.49it/s, train loss 4.286 ; val loss 3.752; lr 1.00e-05]\n",
      "38/300: 100%|███| 135/135 [00:00<00:00, 624.03it/s, train loss 11.549 ; val loss 4.496; lr 1.00e-05]\n",
      "39/300: 100%|███| 135/135 [00:00<00:00, 520.85it/s, train loss 11.833 ; val loss 3.570; lr 1.00e-05]\n",
      "40/300: 100%|████| 135/135 [00:00<00:00, 508.76it/s, train loss 8.863 ; val loss 5.268; lr 1.00e-05]\n",
      "41/300: 100%|████| 135/135 [00:00<00:00, 539.54it/s, train loss 6.323 ; val loss 3.986; lr 1.00e-05]\n",
      "42/300: 100%|████| 135/135 [00:00<00:00, 494.68it/s, train loss 4.421 ; val loss 3.523; lr 1.00e-05]\n",
      "43/300: 100%|████| 135/135 [00:00<00:00, 532.25it/s, train loss 4.028 ; val loss 5.046; lr 1.00e-05]\n",
      "44/300: 100%|███| 135/135 [00:00<00:00, 509.79it/s, train loss 15.379 ; val loss 3.624; lr 1.00e-05]\n",
      "45/300: 100%|████| 135/135 [00:00<00:00, 481.90it/s, train loss 8.595 ; val loss 4.944; lr 1.00e-05]\n",
      "46/300: 100%|████| 135/135 [00:00<00:00, 550.61it/s, train loss 3.450 ; val loss 3.336; lr 1.00e-05]\n",
      "47/300: 100%|███| 135/135 [00:00<00:00, 536.93it/s, train loss 18.528 ; val loss 9.768; lr 1.00e-05]\n",
      "48/300: 100%|████| 135/135 [00:00<00:00, 480.20it/s, train loss 4.887 ; val loss 3.093; lr 1.00e-05]\n",
      "49/300: 100%|████| 135/135 [00:00<00:00, 537.72it/s, train loss 5.035 ; val loss 3.302; lr 1.00e-05]\n",
      "50/300: 100%|███| 135/135 [00:00<00:00, 490.58it/s, train loss 19.420 ; val loss 2.737; lr 1.00e-05]\n",
      "51/300: 100%|████| 135/135 [00:00<00:00, 518.62it/s, train loss 7.444 ; val loss 3.110; lr 1.00e-05]\n",
      "52/300: 100%|████| 135/135 [00:00<00:00, 561.73it/s, train loss 9.973 ; val loss 2.887; lr 1.00e-05]\n",
      "53/300: 100%|████| 135/135 [00:00<00:00, 659.73it/s, train loss 3.219 ; val loss 2.946; lr 1.00e-05]\n",
      "54/300: 100%|███| 135/135 [00:00<00:00, 674.85it/s, train loss 11.734 ; val loss 2.841; lr 1.00e-05]\n",
      "55/300: 100%|███| 135/135 [00:00<00:00, 685.85it/s, train loss 18.880 ; val loss 3.159; lr 1.00e-05]\n",
      "56/300: 100%|████| 135/135 [00:00<00:00, 677.22it/s, train loss 8.336 ; val loss 2.795; lr 1.00e-05]\n",
      "57/300: 100%|████| 135/135 [00:00<00:00, 647.10it/s, train loss 6.066 ; val loss 3.919; lr 1.00e-05]\n",
      "58/300: 100%|████| 135/135 [00:00<00:00, 641.17it/s, train loss 3.850 ; val loss 2.883; lr 1.00e-05]\n",
      "59/300: 100%|████| 135/135 [00:00<00:00, 534.14it/s, train loss 4.986 ; val loss 3.136; lr 1.00e-05]\n",
      "60/300: 100%|████| 135/135 [00:00<00:00, 479.76it/s, train loss 2.781 ; val loss 2.945; lr 1.00e-05]\n",
      "61/300: 100%|████| 135/135 [00:00<00:00, 501.21it/s, train loss 3.321 ; val loss 2.917; lr 1.00e-05]\n",
      "62/300: 100%|████| 135/135 [00:00<00:00, 493.91it/s, train loss 8.490 ; val loss 2.801; lr 1.00e-05]\n",
      "63/300: 100%|████| 135/135 [00:00<00:00, 519.18it/s, train loss 2.966 ; val loss 6.071; lr 1.00e-05]\n",
      "64/300: 100%|████| 135/135 [00:00<00:00, 659.56it/s, train loss 5.592 ; val loss 3.283; lr 1.00e-05]\n",
      "65/300: 100%|████| 135/135 [00:00<00:00, 533.74it/s, train loss 6.704 ; val loss 2.894; lr 1.00e-05]\n",
      "66/300: 100%|████| 135/135 [00:00<00:00, 187.31it/s, train loss 2.536 ; val loss 3.858; lr 1.00e-05]\n",
      "67/300: 100%|████| 135/135 [00:00<00:00, 456.65it/s, train loss 4.394 ; val loss 2.731; lr 1.00e-05]\n",
      "68/300: 100%|████| 135/135 [00:00<00:00, 436.32it/s, train loss 5.979 ; val loss 2.796; lr 1.00e-05]\n",
      "69/300: 100%|███| 135/135 [00:00<00:00, 492.57it/s, train loss 10.849 ; val loss 3.416; lr 1.00e-05]\n",
      "70/300: 100%|████| 135/135 [00:00<00:00, 488.18it/s, train loss 8.748 ; val loss 2.884; lr 1.00e-05]\n",
      "71/300: 100%|████| 135/135 [00:00<00:00, 466.83it/s, train loss 1.229 ; val loss 3.297; lr 1.00e-05]\n",
      "72/300: 100%|████| 135/135 [00:00<00:00, 443.94it/s, train loss 7.560 ; val loss 2.731; lr 1.00e-05]\n",
      "73/300: 100%|████| 135/135 [00:00<00:00, 569.62it/s, train loss 2.520 ; val loss 3.407; lr 1.00e-05]\n",
      "74/300: 100%|███| 135/135 [00:00<00:00, 616.57it/s, train loss 28.412 ; val loss 2.511; lr 1.00e-05]\n",
      "75/300: 100%|███| 135/135 [00:00<00:00, 612.13it/s, train loss 11.303 ; val loss 2.825; lr 1.00e-05]\n",
      "76/300: 100%|████| 135/135 [00:00<00:00, 593.66it/s, train loss 3.665 ; val loss 2.805; lr 1.00e-05]\n",
      "77/300: 100%|███| 135/135 [00:00<00:00, 632.00it/s, train loss 11.914 ; val loss 2.587; lr 1.00e-05]\n",
      "78/300: 100%|████| 135/135 [00:00<00:00, 663.94it/s, train loss 9.011 ; val loss 3.189; lr 1.00e-05]\n",
      "79/300: 100%|████| 135/135 [00:00<00:00, 616.35it/s, train loss 4.757 ; val loss 3.235; lr 1.00e-05]\n",
      "80/300: 100%|████| 135/135 [00:00<00:00, 619.24it/s, train loss 3.383 ; val loss 2.417; lr 1.00e-05]\n",
      "81/300: 100%|████| 135/135 [00:00<00:00, 612.76it/s, train loss 9.208 ; val loss 3.883; lr 1.00e-05]\n",
      "82/300: 100%|████| 135/135 [00:00<00:00, 619.75it/s, train loss 5.014 ; val loss 2.457; lr 1.00e-05]\n",
      "83/300: 100%|████| 135/135 [00:00<00:00, 588.45it/s, train loss 3.328 ; val loss 2.957; lr 1.00e-05]\n",
      "84/300: 100%|████| 135/135 [00:00<00:00, 616.69it/s, train loss 9.806 ; val loss 2.857; lr 1.00e-05]\n",
      "85/300: 100%|████| 135/135 [00:00<00:00, 662.89it/s, train loss 3.413 ; val loss 2.720; lr 1.00e-05]\n",
      "86/300: 100%|███| 135/135 [00:00<00:00, 669.61it/s, train loss 18.089 ; val loss 2.360; lr 1.00e-05]\n",
      "87/300: 100%|████| 135/135 [00:00<00:00, 679.07it/s, train loss 6.331 ; val loss 3.733; lr 1.00e-05]\n",
      "88/300: 100%|███| 135/135 [00:00<00:00, 604.09it/s, train loss 16.377 ; val loss 4.066; lr 1.00e-05]\n",
      "89/300: 100%|███| 135/135 [00:00<00:00, 660.77it/s, train loss 15.536 ; val loss 2.826; lr 1.00e-05]\n",
      "90/300: 100%|████| 135/135 [00:00<00:00, 645.27it/s, train loss 5.903 ; val loss 3.527; lr 1.00e-05]\n",
      "91/300: 100%|████| 135/135 [00:00<00:00, 621.72it/s, train loss 4.285 ; val loss 3.023; lr 1.00e-05]\n",
      "92/300: 100%|████| 135/135 [00:00<00:00, 455.85it/s, train loss 2.671 ; val loss 2.987; lr 1.00e-05]\n",
      "93/300: 100%|███| 135/135 [00:00<00:00, 626.35it/s, train loss 23.105 ; val loss 2.377; lr 1.00e-05]\n",
      "94/300: 100%|████| 135/135 [00:00<00:00, 650.42it/s, train loss 2.138 ; val loss 3.365; lr 1.00e-05]\n",
      "95/300: 100%|████| 135/135 [00:00<00:00, 605.71it/s, train loss 4.727 ; val loss 2.572; lr 1.00e-05]\n",
      "96/300: 100%|███| 135/135 [00:00<00:00, 617.60it/s, train loss 12.934 ; val loss 3.010; lr 1.00e-05]\n",
      "97/300: 100%|████| 135/135 [00:00<00:00, 643.63it/s, train loss 5.109 ; val loss 3.424; lr 1.00e-05]\n",
      "98/300: 100%|████| 135/135 [00:00<00:00, 645.23it/s, train loss 6.047 ; val loss 3.283; lr 1.00e-05]\n",
      "99/300: 100%|███| 135/135 [00:00<00:00, 504.53it/s, train loss 10.107 ; val loss 2.794; lr 1.00e-05]\n",
      "100/300: 100%|███| 135/135 [00:00<00:00, 502.18it/s, train loss 9.179 ; val loss 2.458; lr 1.00e-05]\n",
      "101/300: 100%|██| 135/135 [00:00<00:00, 541.52it/s, train loss 10.225 ; val loss 2.412; lr 1.00e-05]\n",
      "102/300: 100%|███| 135/135 [00:00<00:00, 576.89it/s, train loss 3.347 ; val loss 2.309; lr 1.00e-05]\n",
      "103/300: 100%|███| 135/135 [00:00<00:00, 535.92it/s, train loss 2.685 ; val loss 2.184; lr 1.00e-05]\n",
      "104/300: 100%|███| 135/135 [00:00<00:00, 632.73it/s, train loss 6.273 ; val loss 2.363; lr 1.00e-05]\n",
      "105/300: 100%|███| 135/135 [00:00<00:00, 538.87it/s, train loss 4.283 ; val loss 2.273; lr 1.00e-05]\n",
      "106/300: 100%|███| 135/135 [00:00<00:00, 576.03it/s, train loss 6.069 ; val loss 2.325; lr 1.00e-05]\n",
      "107/300: 100%|███| 135/135 [00:00<00:00, 652.94it/s, train loss 5.338 ; val loss 2.207; lr 1.00e-05]\n",
      "108/300: 100%|███| 135/135 [00:00<00:00, 680.12it/s, train loss 4.721 ; val loss 2.421; lr 1.00e-05]\n",
      "109/300: 100%|███| 135/135 [00:00<00:00, 686.61it/s, train loss 5.406 ; val loss 2.217; lr 1.00e-05]\n",
      "110/300: 100%|███| 135/135 [00:00<00:00, 679.92it/s, train loss 4.109 ; val loss 2.081; lr 1.00e-05]\n",
      "111/300: 100%|███| 135/135 [00:00<00:00, 586.71it/s, train loss 2.634 ; val loss 3.075; lr 1.00e-05]\n",
      "112/300: 100%|███| 135/135 [00:00<00:00, 507.21it/s, train loss 1.028 ; val loss 2.133; lr 1.00e-05]\n",
      "113/300: 100%|███| 135/135 [00:00<00:00, 545.13it/s, train loss 3.736 ; val loss 2.549; lr 1.00e-05]\n",
      "114/300: 100%|██| 135/135 [00:00<00:00, 600.09it/s, train loss 22.427 ; val loss 2.347; lr 1.00e-05]\n",
      "115/300: 100%|██| 135/135 [00:00<00:00, 651.44it/s, train loss 12.525 ; val loss 2.099; lr 1.00e-05]\n",
      "116/300: 100%|███| 135/135 [00:00<00:00, 545.40it/s, train loss 7.052 ; val loss 2.679; lr 1.00e-05]\n",
      "117/300: 100%|███| 135/135 [00:00<00:00, 520.77it/s, train loss 3.943 ; val loss 2.684; lr 1.00e-05]\n",
      "118/300: 100%|███| 135/135 [00:00<00:00, 526.25it/s, train loss 2.773 ; val loss 2.841; lr 1.00e-05]\n",
      "119/300: 100%|███| 135/135 [00:00<00:00, 635.53it/s, train loss 3.784 ; val loss 2.508; lr 1.00e-05]\n",
      "120/300: 100%|██| 135/135 [00:00<00:00, 685.15it/s, train loss 16.640 ; val loss 2.379; lr 1.00e-05]\n",
      "121/300: 100%|███| 135/135 [00:00<00:00, 665.46it/s, train loss 3.095 ; val loss 2.148; lr 1.00e-05]\n",
      "122/300: 100%|███| 135/135 [00:00<00:00, 526.61it/s, train loss 3.234 ; val loss 2.481; lr 1.00e-05]\n",
      "123/300: 100%|███| 135/135 [00:00<00:00, 495.24it/s, train loss 4.072 ; val loss 2.153; lr 1.00e-05]\n",
      "124/300: 100%|██| 135/135 [00:00<00:00, 515.46it/s, train loss 19.715 ; val loss 2.107; lr 1.00e-05]\n",
      "125/300: 100%|██| 135/135 [00:00<00:00, 486.71it/s, train loss 18.657 ; val loss 2.908; lr 1.00e-05]\n",
      "126/300: 100%|███| 135/135 [00:00<00:00, 452.67it/s, train loss 4.504 ; val loss 2.329; lr 1.00e-05]\n",
      "127/300: 100%|███| 135/135 [00:00<00:00, 579.18it/s, train loss 2.347 ; val loss 2.536; lr 1.00e-05]\n",
      "128/300: 100%|███| 135/135 [00:00<00:00, 607.50it/s, train loss 2.670 ; val loss 2.292; lr 1.00e-05]\n",
      "129/300: 100%|██| 135/135 [00:00<00:00, 531.95it/s, train loss 11.467 ; val loss 2.954; lr 1.00e-05]\n",
      "130/300: 100%|███| 135/135 [00:00<00:00, 657.96it/s, train loss 3.744 ; val loss 2.120; lr 1.00e-05]\n",
      "131/300: 100%|███| 135/135 [00:00<00:00, 554.13it/s, train loss 4.737 ; val loss 2.119; lr 1.00e-05]\n",
      "132/300: 100%|███| 135/135 [00:00<00:00, 526.88it/s, train loss 5.255 ; val loss 1.986; lr 1.00e-05]\n",
      "133/300: 100%|███| 135/135 [00:00<00:00, 458.94it/s, train loss 4.745 ; val loss 2.382; lr 1.00e-05]\n",
      "134/300: 100%|██| 135/135 [00:00<00:00, 531.62it/s, train loss 19.162 ; val loss 2.747; lr 1.00e-05]\n",
      "135/300: 100%|███| 135/135 [00:00<00:00, 647.55it/s, train loss 2.049 ; val loss 2.033; lr 1.00e-05]\n",
      "136/300: 100%|███| 135/135 [00:00<00:00, 511.38it/s, train loss 5.582 ; val loss 2.341; lr 1.00e-05]\n",
      "137/300: 100%|███| 135/135 [00:00<00:00, 560.06it/s, train loss 3.614 ; val loss 2.207; lr 1.00e-05]\n",
      "138/300: 100%|███| 135/135 [00:00<00:00, 657.58it/s, train loss 4.929 ; val loss 1.979; lr 1.00e-05]\n",
      "139/300: 100%|██| 135/135 [00:00<00:00, 478.76it/s, train loss 16.665 ; val loss 2.268; lr 1.00e-05]\n",
      "140/300: 100%|██| 135/135 [00:00<00:00, 545.93it/s, train loss 13.090 ; val loss 2.080; lr 1.00e-05]\n",
      "141/300: 100%|██| 135/135 [00:00<00:00, 506.62it/s, train loss 10.080 ; val loss 2.024; lr 1.00e-05]\n",
      "142/300: 100%|██| 135/135 [00:00<00:00, 524.94it/s, train loss 10.084 ; val loss 2.198; lr 1.00e-05]\n",
      "143/300: 100%|███| 135/135 [00:00<00:00, 609.97it/s, train loss 6.296 ; val loss 1.954; lr 1.00e-05]\n",
      "144/300: 100%|███| 135/135 [00:00<00:00, 510.17it/s, train loss 1.844 ; val loss 2.119; lr 1.00e-05]\n",
      "145/300: 100%|███| 135/135 [00:00<00:00, 611.68it/s, train loss 7.275 ; val loss 2.014; lr 1.00e-05]\n",
      "146/300: 100%|██| 135/135 [00:00<00:00, 490.95it/s, train loss 14.000 ; val loss 2.996; lr 1.00e-05]\n",
      "147/300: 100%|███| 135/135 [00:00<00:00, 669.63it/s, train loss 2.794 ; val loss 2.085; lr 1.00e-05]\n",
      "148/300: 100%|███| 135/135 [00:00<00:00, 208.64it/s, train loss 2.065 ; val loss 2.017; lr 1.00e-05]\n",
      "149/300: 100%|██| 135/135 [00:00<00:00, 585.36it/s, train loss 11.947 ; val loss 1.928; lr 1.00e-05]\n",
      "150/300: 100%|██| 135/135 [00:00<00:00, 617.08it/s, train loss 19.479 ; val loss 2.812; lr 1.00e-05]\n",
      "151/300: 100%|███| 135/135 [00:00<00:00, 658.15it/s, train loss 6.111 ; val loss 1.874; lr 1.00e-05]\n",
      "152/300: 100%|██| 135/135 [00:00<00:00, 608.02it/s, train loss 19.916 ; val loss 1.903; lr 1.00e-05]\n",
      "153/300: 100%|██| 135/135 [00:00<00:00, 607.71it/s, train loss 17.168 ; val loss 2.792; lr 1.00e-05]\n",
      "154/300: 100%|███| 135/135 [00:00<00:00, 618.31it/s, train loss 2.287 ; val loss 1.817; lr 1.00e-05]\n",
      "155/300: 100%|███| 135/135 [00:00<00:00, 626.45it/s, train loss 3.852 ; val loss 3.200; lr 1.00e-05]\n",
      "156/300: 100%|███| 135/135 [00:00<00:00, 600.33it/s, train loss 5.369 ; val loss 1.953; lr 1.00e-05]\n",
      "157/300: 100%|███| 135/135 [00:00<00:00, 612.73it/s, train loss 6.068 ; val loss 1.871; lr 1.00e-05]\n",
      "158/300: 100%|███| 135/135 [00:00<00:00, 610.76it/s, train loss 3.701 ; val loss 2.230; lr 1.00e-05]\n",
      "159/300: 100%|███| 135/135 [00:00<00:00, 642.94it/s, train loss 5.277 ; val loss 2.192; lr 1.00e-05]\n",
      "160/300: 100%|██| 135/135 [00:00<00:00, 645.15it/s, train loss 15.419 ; val loss 2.095; lr 1.00e-05]\n",
      "161/300: 100%|███| 135/135 [00:00<00:00, 661.17it/s, train loss 5.212 ; val loss 2.003; lr 1.00e-05]\n",
      "162/300: 100%|███| 135/135 [00:00<00:00, 670.03it/s, train loss 9.492 ; val loss 2.232; lr 1.00e-05]\n",
      "163/300: 100%|███| 135/135 [00:00<00:00, 669.08it/s, train loss 3.892 ; val loss 2.264; lr 1.00e-05]\n",
      "164/300: 100%|██| 135/135 [00:00<00:00, 674.82it/s, train loss 11.088 ; val loss 2.116; lr 1.00e-05]\n",
      "165/300: 100%|███| 135/135 [00:00<00:00, 660.02it/s, train loss 5.912 ; val loss 2.055; lr 1.00e-05]\n",
      "166/300: 100%|███| 135/135 [00:00<00:00, 640.10it/s, train loss 3.173 ; val loss 1.799; lr 1.00e-05]\n",
      "167/300: 100%|██| 135/135 [00:00<00:00, 618.95it/s, train loss 12.381 ; val loss 2.370; lr 1.00e-05]\n",
      "168/300: 100%|██| 135/135 [00:00<00:00, 619.54it/s, train loss 14.885 ; val loss 1.856; lr 1.00e-05]\n",
      "169/300: 100%|███| 135/135 [00:00<00:00, 603.69it/s, train loss 4.519 ; val loss 1.814; lr 1.00e-05]\n",
      "170/300: 100%|███| 135/135 [00:00<00:00, 537.31it/s, train loss 4.770 ; val loss 1.888; lr 1.00e-05]\n",
      "171/300: 100%|███| 135/135 [00:00<00:00, 604.28it/s, train loss 2.673 ; val loss 2.227; lr 1.00e-05]\n",
      "172/300: 100%|██| 135/135 [00:00<00:00, 608.17it/s, train loss 12.496 ; val loss 1.737; lr 1.00e-05]\n",
      "173/300: 100%|███| 135/135 [00:00<00:00, 626.36it/s, train loss 2.169 ; val loss 1.940; lr 1.00e-05]\n",
      "174/300: 100%|███| 135/135 [00:00<00:00, 639.25it/s, train loss 3.943 ; val loss 1.741; lr 1.00e-05]\n",
      "175/300: 100%|███| 135/135 [00:00<00:00, 648.41it/s, train loss 4.070 ; val loss 2.380; lr 1.00e-05]\n",
      "176/300: 100%|██| 135/135 [00:00<00:00, 673.68it/s, train loss 13.628 ; val loss 1.937; lr 1.00e-05]\n",
      "177/300: 100%|███| 135/135 [00:00<00:00, 661.71it/s, train loss 9.169 ; val loss 1.834; lr 1.00e-05]\n",
      "178/300: 100%|███| 135/135 [00:00<00:00, 660.63it/s, train loss 3.679 ; val loss 1.824; lr 1.00e-05]\n",
      "179/300: 100%|███| 135/135 [00:00<00:00, 655.27it/s, train loss 5.089 ; val loss 1.913; lr 1.00e-05]\n",
      "180/300: 100%|███| 135/135 [00:00<00:00, 617.43it/s, train loss 4.479 ; val loss 2.034; lr 1.00e-05]\n",
      "181/300: 100%|███| 135/135 [00:00<00:00, 587.58it/s, train loss 3.547 ; val loss 2.179; lr 1.00e-05]\n",
      "182/300: 100%|███| 135/135 [00:00<00:00, 575.48it/s, train loss 8.493 ; val loss 1.691; lr 1.00e-05]\n",
      "183/300: 100%|███| 135/135 [00:00<00:00, 609.52it/s, train loss 2.224 ; val loss 1.812; lr 1.00e-05]\n",
      "184/300: 100%|███| 135/135 [00:00<00:00, 620.88it/s, train loss 2.046 ; val loss 1.696; lr 1.00e-05]\n",
      "185/300: 100%|██| 135/135 [00:00<00:00, 614.74it/s, train loss 10.860 ; val loss 2.040; lr 1.00e-05]\n",
      "186/300: 100%|███| 135/135 [00:00<00:00, 620.06it/s, train loss 8.732 ; val loss 1.771; lr 1.00e-05]\n",
      "187/300: 100%|██| 135/135 [00:00<00:00, 658.52it/s, train loss 17.267 ; val loss 1.731; lr 1.00e-05]\n",
      "188/300: 100%|██| 135/135 [00:00<00:00, 645.75it/s, train loss 11.134 ; val loss 1.785; lr 1.00e-05]\n",
      "189/300: 100%|███| 135/135 [00:00<00:00, 661.69it/s, train loss 7.440 ; val loss 1.691; lr 1.00e-05]\n",
      "190/300: 100%|███| 135/135 [00:00<00:00, 682.06it/s, train loss 3.398 ; val loss 1.758; lr 1.00e-05]\n",
      "191/300: 100%|███| 135/135 [00:00<00:00, 672.47it/s, train loss 6.027 ; val loss 1.752; lr 1.00e-05]\n",
      "192/300: 100%|███| 135/135 [00:00<00:00, 673.25it/s, train loss 6.894 ; val loss 1.736; lr 1.00e-05]\n",
      "193/300: 100%|███| 135/135 [00:00<00:00, 673.82it/s, train loss 2.298 ; val loss 1.704; lr 1.00e-05]\n",
      "194/300: 100%|███| 135/135 [00:00<00:00, 652.27it/s, train loss 2.818 ; val loss 1.927; lr 1.00e-05]\n",
      "195/300: 100%|███| 135/135 [00:00<00:00, 666.48it/s, train loss 3.329 ; val loss 2.067; lr 1.00e-05]\n",
      "196/300: 100%|███| 135/135 [00:00<00:00, 657.86it/s, train loss 4.772 ; val loss 1.705; lr 1.00e-05]\n",
      "197/300: 100%|███| 135/135 [00:00<00:00, 663.22it/s, train loss 3.134 ; val loss 1.708; lr 1.00e-05]\n",
      "198/300: 100%|███| 135/135 [00:00<00:00, 677.56it/s, train loss 4.416 ; val loss 1.689; lr 1.00e-05]\n",
      "199/300: 100%|███| 135/135 [00:00<00:00, 655.45it/s, train loss 2.634 ; val loss 1.698; lr 1.00e-05]\n",
      "200/300: 100%|███| 135/135 [00:00<00:00, 667.77it/s, train loss 7.401 ; val loss 2.078; lr 1.00e-05]\n",
      "201/300: 100%|███| 135/135 [00:00<00:00, 660.91it/s, train loss 2.913 ; val loss 1.678; lr 1.00e-05]\n",
      "202/300: 100%|███| 135/135 [00:00<00:00, 667.57it/s, train loss 3.503 ; val loss 1.699; lr 1.00e-05]\n",
      "203/300: 100%|███| 135/135 [00:00<00:00, 660.31it/s, train loss 8.751 ; val loss 1.911; lr 1.00e-05]\n",
      "204/300: 100%|███| 135/135 [00:00<00:00, 658.43it/s, train loss 2.803 ; val loss 2.050; lr 1.00e-05]\n",
      "205/300: 100%|███| 135/135 [00:00<00:00, 670.48it/s, train loss 9.401 ; val loss 1.812; lr 1.00e-05]\n",
      "206/300: 100%|███| 135/135 [00:00<00:00, 651.65it/s, train loss 3.985 ; val loss 1.794; lr 1.00e-05]\n",
      "207/300: 100%|███| 135/135 [00:00<00:00, 661.58it/s, train loss 1.642 ; val loss 2.156; lr 1.00e-05]\n",
      "208/300: 100%|██| 135/135 [00:00<00:00, 660.63it/s, train loss 10.822 ; val loss 1.642; lr 1.00e-05]\n",
      "209/300: 100%|███| 135/135 [00:00<00:00, 651.18it/s, train loss 3.256 ; val loss 1.709; lr 1.00e-05]\n",
      "210/300: 100%|███| 135/135 [00:00<00:00, 672.11it/s, train loss 8.481 ; val loss 1.810; lr 1.00e-05]\n",
      "211/300: 100%|███| 135/135 [00:00<00:00, 638.02it/s, train loss 4.531 ; val loss 1.649; lr 1.00e-05]\n",
      "212/300: 100%|███| 135/135 [00:00<00:00, 599.51it/s, train loss 2.438 ; val loss 1.630; lr 1.00e-05]\n",
      "213/300: 100%|███| 135/135 [00:00<00:00, 670.48it/s, train loss 7.572 ; val loss 2.824; lr 1.00e-05]\n",
      "214/300: 100%|███| 135/135 [00:00<00:00, 678.95it/s, train loss 8.407 ; val loss 1.915; lr 1.00e-05]\n",
      "215/300: 100%|███| 135/135 [00:00<00:00, 675.68it/s, train loss 8.645 ; val loss 1.759; lr 1.00e-05]\n",
      "216/300: 100%|███| 135/135 [00:00<00:00, 677.60it/s, train loss 1.019 ; val loss 1.686; lr 1.00e-05]\n",
      "217/300: 100%|███| 135/135 [00:00<00:00, 671.20it/s, train loss 1.298 ; val loss 2.391; lr 1.00e-05]\n",
      "218/300: 100%|███| 135/135 [00:00<00:00, 671.35it/s, train loss 4.334 ; val loss 1.572; lr 1.00e-05]\n",
      "219/300: 100%|███| 135/135 [00:00<00:00, 673.26it/s, train loss 5.098 ; val loss 1.679; lr 1.00e-05]\n",
      "220/300: 100%|███| 135/135 [00:00<00:00, 671.84it/s, train loss 2.247 ; val loss 1.632; lr 1.00e-05]\n",
      "221/300: 100%|██| 135/135 [00:00<00:00, 664.20it/s, train loss 11.944 ; val loss 1.862; lr 1.00e-05]\n",
      "222/300: 100%|██| 135/135 [00:00<00:00, 589.07it/s, train loss 11.226 ; val loss 1.606; lr 1.00e-05]\n",
      "223/300: 100%|██| 135/135 [00:00<00:00, 210.34it/s, train loss 18.366 ; val loss 2.046; lr 1.00e-05]\n",
      "224/300: 100%|███| 135/135 [00:00<00:00, 567.21it/s, train loss 7.199 ; val loss 1.692; lr 1.00e-05]\n",
      "225/300: 100%|██| 135/135 [00:00<00:00, 616.01it/s, train loss 15.225 ; val loss 1.653; lr 1.00e-05]\n",
      "226/300: 100%|███| 135/135 [00:00<00:00, 630.40it/s, train loss 6.631 ; val loss 1.871; lr 1.00e-05]\n",
      "227/300: 100%|███| 135/135 [00:00<00:00, 607.09it/s, train loss 2.692 ; val loss 2.000; lr 1.00e-05]\n",
      "228/300: 100%|██| 135/135 [00:00<00:00, 616.60it/s, train loss 30.695 ; val loss 2.049; lr 1.00e-05]\n",
      "229/300: 100%|███| 135/135 [00:00<00:00, 630.18it/s, train loss 2.884 ; val loss 1.549; lr 1.00e-05]\n",
      "230/300: 100%|███| 135/135 [00:00<00:00, 650.07it/s, train loss 1.884 ; val loss 1.566; lr 1.00e-05]\n",
      "231/300: 100%|███| 135/135 [00:00<00:00, 652.16it/s, train loss 4.543 ; val loss 1.640; lr 1.00e-05]\n",
      "232/300: 100%|███| 135/135 [00:00<00:00, 662.08it/s, train loss 2.526 ; val loss 1.722; lr 1.00e-05]\n",
      "233/300: 100%|███| 135/135 [00:00<00:00, 657.69it/s, train loss 3.173 ; val loss 1.730; lr 1.00e-05]\n",
      "234/300: 100%|██| 135/135 [00:00<00:00, 654.25it/s, train loss 11.472 ; val loss 2.033; lr 1.00e-05]\n",
      "235/300: 100%|███| 135/135 [00:00<00:00, 663.25it/s, train loss 2.379 ; val loss 1.554; lr 1.00e-05]\n",
      "236/300: 100%|███| 135/135 [00:00<00:00, 669.06it/s, train loss 1.589 ; val loss 1.517; lr 1.00e-05]\n",
      "237/300: 100%|███| 135/135 [00:00<00:00, 673.53it/s, train loss 2.878 ; val loss 1.536; lr 1.00e-05]\n",
      "238/300: 100%|███| 135/135 [00:00<00:00, 674.56it/s, train loss 6.115 ; val loss 1.999; lr 1.00e-05]\n",
      "239/300: 100%|███| 135/135 [00:00<00:00, 671.32it/s, train loss 3.413 ; val loss 1.740; lr 1.00e-05]\n",
      "240/300: 100%|███| 135/135 [00:00<00:00, 667.73it/s, train loss 4.684 ; val loss 1.643; lr 1.00e-05]\n",
      "241/300: 100%|███| 135/135 [00:00<00:00, 651.67it/s, train loss 8.597 ; val loss 1.735; lr 1.00e-05]\n",
      "242/300: 100%|███| 135/135 [00:00<00:00, 654.99it/s, train loss 4.735 ; val loss 2.274; lr 1.00e-05]\n",
      "243/300: 100%|███| 135/135 [00:00<00:00, 675.39it/s, train loss 2.350 ; val loss 1.531; lr 1.00e-05]\n",
      "244/300: 100%|███| 135/135 [00:00<00:00, 673.16it/s, train loss 4.907 ; val loss 1.592; lr 1.00e-05]\n",
      "245/300: 100%|███| 135/135 [00:00<00:00, 652.87it/s, train loss 4.976 ; val loss 2.213; lr 1.00e-05]\n",
      "246/300: 100%|███| 135/135 [00:00<00:00, 666.46it/s, train loss 5.482 ; val loss 1.493; lr 1.00e-05]\n",
      "247/300: 100%|███| 135/135 [00:00<00:00, 652.89it/s, train loss 3.906 ; val loss 1.627; lr 1.00e-05]\n",
      "248/300: 100%|███| 135/135 [00:00<00:00, 670.62it/s, train loss 3.295 ; val loss 1.622; lr 1.00e-05]\n",
      "249/300: 100%|██| 135/135 [00:00<00:00, 672.64it/s, train loss 17.378 ; val loss 1.540; lr 1.00e-05]\n",
      "250/300: 100%|███| 135/135 [00:00<00:00, 665.33it/s, train loss 3.511 ; val loss 1.507; lr 1.00e-05]\n",
      "251/300: 100%|███| 135/135 [00:00<00:00, 660.11it/s, train loss 6.895 ; val loss 1.538; lr 1.00e-05]\n",
      "252/300: 100%|███| 135/135 [00:00<00:00, 645.71it/s, train loss 2.524 ; val loss 1.979; lr 1.00e-05]\n",
      "253/300: 100%|███| 135/135 [00:00<00:00, 586.70it/s, train loss 3.699 ; val loss 1.690; lr 1.00e-05]\n",
      "254/300: 100%|███| 135/135 [00:00<00:00, 622.14it/s, train loss 2.941 ; val loss 1.524; lr 1.00e-05]\n",
      "255/300: 100%|███| 135/135 [00:00<00:00, 627.21it/s, train loss 1.483 ; val loss 1.739; lr 1.00e-05]\n",
      "256/300: 100%|███| 135/135 [00:00<00:00, 604.84it/s, train loss 3.253 ; val loss 2.015; lr 1.00e-05]\n",
      "257/300: 100%|███| 135/135 [00:00<00:00, 607.24it/s, train loss 2.835 ; val loss 1.719; lr 1.00e-05]\n",
      "258/300: 100%|██| 135/135 [00:00<00:00, 606.18it/s, train loss 11.925 ; val loss 1.632; lr 1.00e-05]\n",
      "259/300: 100%|███| 135/135 [00:00<00:00, 617.66it/s, train loss 4.939 ; val loss 1.493; lr 1.00e-05]\n",
      "260/300: 100%|██| 135/135 [00:00<00:00, 647.93it/s, train loss 11.399 ; val loss 1.498; lr 1.00e-05]\n",
      "261/300: 100%|███| 135/135 [00:00<00:00, 651.90it/s, train loss 7.874 ; val loss 1.576; lr 1.00e-05]\n",
      "262/300: 100%|███| 135/135 [00:00<00:00, 668.76it/s, train loss 7.131 ; val loss 1.476; lr 1.00e-05]\n",
      "263/300: 100%|███| 135/135 [00:00<00:00, 673.22it/s, train loss 2.999 ; val loss 1.579; lr 1.00e-05]\n",
      "264/300: 100%|███| 135/135 [00:00<00:00, 663.93it/s, train loss 3.900 ; val loss 1.611; lr 1.00e-05]\n",
      "265/300: 100%|███| 135/135 [00:00<00:00, 671.02it/s, train loss 4.763 ; val loss 1.512; lr 1.00e-05]\n",
      "266/300: 100%|███| 135/135 [00:00<00:00, 620.55it/s, train loss 9.652 ; val loss 1.749; lr 1.00e-05]\n",
      "267/300: 100%|██| 135/135 [00:00<00:00, 632.82it/s, train loss 14.214 ; val loss 1.688; lr 1.00e-05]\n",
      "268/300: 100%|███| 135/135 [00:00<00:00, 611.60it/s, train loss 1.569 ; val loss 1.678; lr 1.00e-05]\n",
      "269/300: 100%|███| 135/135 [00:00<00:00, 604.94it/s, train loss 2.532 ; val loss 1.512; lr 1.00e-05]\n",
      "270/300: 100%|███| 135/135 [00:00<00:00, 621.21it/s, train loss 1.245 ; val loss 1.543; lr 1.00e-05]\n",
      "271/300: 100%|███| 135/135 [00:00<00:00, 614.13it/s, train loss 3.330 ; val loss 1.514; lr 1.00e-05]\n",
      "272/300: 100%|██| 135/135 [00:00<00:00, 547.36it/s, train loss 11.771 ; val loss 1.628; lr 1.00e-05]\n",
      "273/300: 100%|██| 135/135 [00:00<00:00, 617.50it/s, train loss 11.797 ; val loss 1.895; lr 1.00e-05]\n",
      "274/300: 100%|███| 135/135 [00:00<00:00, 640.16it/s, train loss 2.038 ; val loss 1.545; lr 1.00e-05]\n",
      "275/300: 100%|███| 135/135 [00:00<00:00, 217.50it/s, train loss 2.122 ; val loss 1.463; lr 1.00e-05]\n",
      "276/300: 100%|███| 135/135 [00:00<00:00, 641.39it/s, train loss 3.713 ; val loss 1.596; lr 1.00e-05]\n",
      "277/300: 100%|███| 135/135 [00:00<00:00, 641.54it/s, train loss 4.358 ; val loss 1.490; lr 1.00e-05]\n",
      "278/300: 100%|███| 135/135 [00:00<00:00, 639.77it/s, train loss 1.931 ; val loss 1.815; lr 1.00e-05]\n",
      "279/300: 100%|██| 135/135 [00:00<00:00, 615.12it/s, train loss 14.685 ; val loss 1.525; lr 1.00e-05]\n",
      "280/300: 100%|███| 135/135 [00:00<00:00, 594.98it/s, train loss 3.108 ; val loss 1.986; lr 1.00e-05]\n",
      "281/300: 100%|██| 135/135 [00:00<00:00, 615.86it/s, train loss 11.141 ; val loss 1.927; lr 1.00e-05]\n",
      "282/300: 100%|███| 135/135 [00:00<00:00, 617.32it/s, train loss 3.869 ; val loss 1.527; lr 1.00e-05]\n",
      "283/300: 100%|███| 135/135 [00:00<00:00, 625.47it/s, train loss 3.910 ; val loss 1.501; lr 1.00e-05]\n",
      "284/300: 100%|███| 135/135 [00:00<00:00, 620.04it/s, train loss 2.375 ; val loss 1.718; lr 1.00e-05]\n",
      "285/300: 100%|███| 135/135 [00:00<00:00, 632.31it/s, train loss 2.778 ; val loss 1.457; lr 1.00e-05]\n",
      "286/300: 100%|███| 135/135 [00:00<00:00, 637.33it/s, train loss 4.471 ; val loss 1.482; lr 1.00e-05]\n",
      "287/300: 100%|███| 135/135 [00:00<00:00, 652.83it/s, train loss 9.873 ; val loss 1.818; lr 1.00e-05]\n",
      "288/300: 100%|██| 135/135 [00:00<00:00, 671.17it/s, train loss 10.722 ; val loss 1.714; lr 1.00e-05]\n",
      "289/300: 100%|███| 135/135 [00:00<00:00, 620.83it/s, train loss 3.378 ; val loss 1.660; lr 1.00e-05]\n",
      "290/300: 100%|███| 135/135 [00:00<00:00, 622.09it/s, train loss 4.325 ; val loss 1.665; lr 1.00e-05]\n",
      "291/300: 100%|██| 135/135 [00:00<00:00, 596.77it/s, train loss 15.327 ; val loss 1.949; lr 1.00e-05]\n",
      "292/300: 100%|███| 135/135 [00:00<00:00, 606.73it/s, train loss 4.318 ; val loss 1.849; lr 1.00e-05]\n",
      "293/300: 100%|███| 135/135 [00:00<00:00, 571.16it/s, train loss 4.819 ; val loss 2.255; lr 1.00e-05]\n",
      "294/300: 100%|███| 135/135 [00:00<00:00, 614.57it/s, train loss 3.485 ; val loss 1.995; lr 1.00e-05]\n",
      "295/300: 100%|███| 135/135 [00:00<00:00, 629.40it/s, train loss 1.672 ; val loss 1.555; lr 1.00e-05]\n",
      "296/300: 100%|███| 135/135 [00:00<00:00, 629.42it/s, train loss 6.375 ; val loss 1.510; lr 1.00e-05]\n",
      "297/300: 100%|███| 135/135 [00:00<00:00, 655.04it/s, train loss 5.555 ; val loss 1.461; lr 1.00e-05]\n",
      "298/300: 100%|███| 135/135 [00:00<00:00, 656.05it/s, train loss 6.435 ; val loss 1.466; lr 1.00e-05]\n",
      "299/300: 100%|███| 135/135 [00:00<00:00, 648.37it/s, train loss 1.997 ; val loss 1.779; lr 1.00e-05]\n",
      "300/300: 100%|███| 135/135 [00:00<00:00, 600.96it/s, train loss 2.681 ; val loss 1.491; lr 1.00e-05]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "total_epoch = 300\n",
    "for epoch in range(total_epoch):\n",
    "    with tqdm(train_loader, total=len(train_loader), ncols=100) as tbar:\n",
    "        tbar.set_description(f\"{epoch+1}/{total_epoch}\")\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            x = x.float().to(device)\n",
    "            preds = model(x).squeeze(dim=1)                \n",
    "            loss = loss_fcn(y.float().to(device), preds)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % len(train_loader) == 0:\n",
    "                tot_mse = []\n",
    "                for j, (x, y) in enumerate(val_loader):\n",
    "                    model.eval()\n",
    "                    preds = model(x.float().to(device)).squeeze(dim=1)\n",
    "                    tot_mse.append(loss_fcn(y.float().to(device), preds).detach().cpu().numpy())\n",
    "                cur_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "                tbar.set_postfix_str(f'train loss {loss.item():.3f} ; val loss {np.mean(tot_mse):.3f}; lr {cur_lr:.2e}')\n",
    "            tbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for j, (x, y) in enumerate(val_loader):\n",
    "#     model.eval()\n",
    "#     preds = model(x.float().to(device)).squeeze(dim=1)\n",
    "#     print(preds, y, sep='\\n')\n",
    "#     print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(test_data_path)\n",
    "test_dataset = test_dataset[test_dataset.columns[1:]]\n",
    "test_dataset = test_dataset[selected_feature_idx[1:]]\n",
    "\n",
    "test_loader = DataLoader(COVID19Dataset(test_dataset.values, None), batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "preds_all = []\n",
    "for x in test_loader:\n",
    "    x = x.float().to(device)\n",
    "    preds = model(x).detach().cpu().numpy().squeeze()\n",
    "    # print(preds)\n",
    "    preds_all.extend(preds)\n",
    "\n",
    "\n",
    "save_pred(preds_all, \"./dl_selected_original_feature.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用所有(feature scaled)数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2159, 34) (540, 34) (2159,) (540,)\n"
     ]
    }
   ],
   "source": [
    "same_seed(77)\n",
    "dataset = pd.read_csv(train_data_path)\n",
    "dataset = dataset[dataset.columns[1:]]  # remove 'id' column\n",
    "\n",
    "feature_process_pipeline = Pipeline([('maxmin_scaler', MaxminScaler('tested_positive'))]) \n",
    "dataset = feature_process_pipeline.transform(dataset.copy())\n",
    "\n",
    "corr_matrix = dataset.corr()\n",
    "target_coor = corr_matrix['tested_positive.4'].sort_values(ascending=False)\n",
    "mask = target_coor > 0.5  # 选择与target相关性大于0.5的feature参与训练\n",
    "selected_feature_idx = list(target_coor.index[mask])\n",
    "\n",
    "x_dataset = dataset[selected_feature_idx[1:]]\n",
    "y_dataset = dataset.iloc[:, -1]\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_dataset, y_dataset, test_size=0.2, random_state=77)  # 划分测试集和验证集\n",
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tested_positive.3</th>\n",
       "      <th>tested_positive.2</th>\n",
       "      <th>tested_positive.1</th>\n",
       "      <th>tested_positive</th>\n",
       "      <th>hh_cmnty_cli.4</th>\n",
       "      <th>hh_cmnty_cli.3</th>\n",
       "      <th>nohh_cmnty_cli.4</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "      <th>nohh_cmnty_cli.3</th>\n",
       "      <th>hh_cmnty_cli.1</th>\n",
       "      <th>...</th>\n",
       "      <th>anxious.4</th>\n",
       "      <th>anxious.3</th>\n",
       "      <th>anxious.2</th>\n",
       "      <th>anxious.1</th>\n",
       "      <th>anxious</th>\n",
       "      <th>work_outside_home.4</th>\n",
       "      <th>work_outside_home.3</th>\n",
       "      <th>work_outside_home.2</th>\n",
       "      <th>work_outside_home.1</th>\n",
       "      <th>work_outside_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>5.720446</td>\n",
       "      <td>5.066153</td>\n",
       "      <td>3.728124</td>\n",
       "      <td>3.217874</td>\n",
       "      <td>0.030578</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.043391</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>0.050363</td>\n",
       "      <td>0.036260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349364</td>\n",
       "      <td>0.315327</td>\n",
       "      <td>0.365267</td>\n",
       "      <td>0.352714</td>\n",
       "      <td>0.315041</td>\n",
       "      <td>0.125020</td>\n",
       "      <td>0.101719</td>\n",
       "      <td>0.108746</td>\n",
       "      <td>0.125541</td>\n",
       "      <td>0.118981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>5.238175</td>\n",
       "      <td>6.132241</td>\n",
       "      <td>8.019026</td>\n",
       "      <td>7.425820</td>\n",
       "      <td>0.123110</td>\n",
       "      <td>0.137675</td>\n",
       "      <td>0.101318</td>\n",
       "      <td>0.140786</td>\n",
       "      <td>0.106964</td>\n",
       "      <td>0.132884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424925</td>\n",
       "      <td>0.448514</td>\n",
       "      <td>0.426557</td>\n",
       "      <td>0.417610</td>\n",
       "      <td>0.469680</td>\n",
       "      <td>0.461224</td>\n",
       "      <td>0.470054</td>\n",
       "      <td>0.477118</td>\n",
       "      <td>0.463857</td>\n",
       "      <td>0.506354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>14.071856</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>13.437500</td>\n",
       "      <td>0.795198</td>\n",
       "      <td>0.790392</td>\n",
       "      <td>0.810744</td>\n",
       "      <td>0.811431</td>\n",
       "      <td>0.795247</td>\n",
       "      <td>0.801507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610920</td>\n",
       "      <td>0.637113</td>\n",
       "      <td>0.735003</td>\n",
       "      <td>0.739021</td>\n",
       "      <td>0.779809</td>\n",
       "      <td>0.432208</td>\n",
       "      <td>0.454133</td>\n",
       "      <td>0.552732</td>\n",
       "      <td>0.601344</td>\n",
       "      <td>0.591493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>12.980782</td>\n",
       "      <td>10.194185</td>\n",
       "      <td>8.715588</td>\n",
       "      <td>9.047263</td>\n",
       "      <td>0.193160</td>\n",
       "      <td>0.193023</td>\n",
       "      <td>0.159913</td>\n",
       "      <td>0.176219</td>\n",
       "      <td>0.154867</td>\n",
       "      <td>0.181626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393338</td>\n",
       "      <td>0.437170</td>\n",
       "      <td>0.466091</td>\n",
       "      <td>0.519418</td>\n",
       "      <td>0.507335</td>\n",
       "      <td>0.498154</td>\n",
       "      <td>0.506570</td>\n",
       "      <td>0.537812</td>\n",
       "      <td>0.517657</td>\n",
       "      <td>0.528373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2.909232</td>\n",
       "      <td>3.374709</td>\n",
       "      <td>3.514739</td>\n",
       "      <td>3.488372</td>\n",
       "      <td>0.060224</td>\n",
       "      <td>0.056132</td>\n",
       "      <td>0.050647</td>\n",
       "      <td>0.057334</td>\n",
       "      <td>0.046105</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290070</td>\n",
       "      <td>0.281768</td>\n",
       "      <td>0.294648</td>\n",
       "      <td>0.268472</td>\n",
       "      <td>0.224005</td>\n",
       "      <td>0.152179</td>\n",
       "      <td>0.156876</td>\n",
       "      <td>0.128048</td>\n",
       "      <td>0.107788</td>\n",
       "      <td>0.110595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tested_positive.3  tested_positive.2  tested_positive.1  \\\n",
       "1611           5.720446           5.066153           3.728124   \n",
       "1738           5.238175           6.132241           8.019026   \n",
       "1156          14.071856          14.583333          14.583333   \n",
       "530           12.980782          10.194185           8.715588   \n",
       "614            2.909232           3.374709           3.514739   \n",
       "\n",
       "      tested_positive  hh_cmnty_cli.4  hh_cmnty_cli.3  nohh_cmnty_cli.4  \\\n",
       "1611         3.217874        0.030578        0.038501          0.043391   \n",
       "1738         7.425820        0.123110        0.137675          0.101318   \n",
       "1156        13.437500        0.795198        0.790392          0.810744   \n",
       "530          9.047263        0.193160        0.193023          0.159913   \n",
       "614          3.488372        0.060224        0.056132          0.050647   \n",
       "\n",
       "      hh_cmnty_cli.2  nohh_cmnty_cli.3  hh_cmnty_cli.1  ...  anxious.4  \\\n",
       "1611        0.035927          0.050363        0.036260  ...   0.349364   \n",
       "1738        0.140786          0.106964        0.132884  ...   0.424925   \n",
       "1156        0.811431          0.795247        0.801507  ...   0.610920   \n",
       "530         0.176219          0.154867        0.181626  ...   0.393338   \n",
       "614         0.057334          0.046105        0.058849  ...   0.290070   \n",
       "\n",
       "      anxious.3  anxious.2  anxious.1   anxious  work_outside_home.4  \\\n",
       "1611   0.315327   0.365267   0.352714  0.315041             0.125020   \n",
       "1738   0.448514   0.426557   0.417610  0.469680             0.461224   \n",
       "1156   0.637113   0.735003   0.739021  0.779809             0.432208   \n",
       "530    0.437170   0.466091   0.519418  0.507335             0.498154   \n",
       "614    0.281768   0.294648   0.268472  0.224005             0.152179   \n",
       "\n",
       "      work_outside_home.3  work_outside_home.2  work_outside_home.1  \\\n",
       "1611             0.101719             0.108746             0.125541   \n",
       "1738             0.470054             0.477118             0.463857   \n",
       "1156             0.454133             0.552732             0.601344   \n",
       "530              0.506570             0.537812             0.517657   \n",
       "614              0.156876             0.128048             0.107788   \n",
       "\n",
       "      work_outside_home  \n",
       "1611           0.118981  \n",
       "1738           0.506354  \n",
       "1156           0.591493  \n",
       "530            0.528373  \n",
       "614            0.110595  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tested_positive.3', 'tested_positive.2', 'tested_positive.1',\n",
       "       'tested_positive', 'hh_cmnty_cli.4', 'hh_cmnty_cli.3',\n",
       "       'nohh_cmnty_cli.4', 'hh_cmnty_cli.2', 'nohh_cmnty_cli.3',\n",
       "       'hh_cmnty_cli.1', 'nohh_cmnty_cli.2', 'hh_cmnty_cli',\n",
       "       'nohh_cmnty_cli.1', 'nohh_cmnty_cli', 'ili.4', 'cli.4', 'ili.3',\n",
       "       'cli.3', 'ili.2', 'cli.2', 'ili.1', 'cli.1', 'ili', 'cli', 'anxious.4',\n",
       "       'anxious.3', 'anxious.2', 'anxious.1', 'anxious', 'work_outside_home.4',\n",
       "       'work_outside_home.3', 'work_outside_home.2', 'work_outside_home.1',\n",
       "       'work_outside_home'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/300: 100%|████| 168/168 [00:00<00:00, 529.81it/s, train loss 17.000 ; val loss 8.537; lr 1.00e-04]\n",
      "2/300: 100%|████| 168/168 [00:00<00:00, 454.77it/s, train loss 16.180 ; val loss 6.300; lr 1.00e-04]\n",
      "3/300: 100%|█████| 168/168 [00:00<00:00, 443.12it/s, train loss 8.894 ; val loss 5.893; lr 1.00e-04]\n",
      "4/300: 100%|█████| 168/168 [00:00<00:00, 406.80it/s, train loss 6.480 ; val loss 4.060; lr 1.00e-04]\n",
      "5/300: 100%|█████| 168/168 [00:00<00:00, 478.45it/s, train loss 9.465 ; val loss 2.196; lr 1.00e-04]\n",
      "6/300: 100%|█████| 168/168 [00:00<00:00, 479.79it/s, train loss 5.034 ; val loss 2.149; lr 1.00e-04]\n",
      "7/300: 100%|█████| 168/168 [00:00<00:00, 481.39it/s, train loss 7.280 ; val loss 1.594; lr 1.00e-04]\n",
      "8/300: 100%|█████| 168/168 [00:00<00:00, 556.01it/s, train loss 4.363 ; val loss 1.467; lr 1.00e-04]\n",
      "9/300: 100%|█████| 168/168 [00:00<00:00, 602.73it/s, train loss 6.542 ; val loss 1.536; lr 1.00e-04]\n",
      "10/300: 100%|████| 168/168 [00:00<00:00, 525.58it/s, train loss 2.678 ; val loss 1.397; lr 1.00e-04]\n",
      "11/300: 100%|████| 168/168 [00:00<00:00, 487.41it/s, train loss 8.487 ; val loss 1.384; lr 1.00e-04]\n",
      "12/300: 100%|████| 168/168 [00:00<00:00, 500.28it/s, train loss 9.770 ; val loss 1.314; lr 1.00e-04]\n",
      "13/300: 100%|████| 168/168 [00:00<00:00, 556.73it/s, train loss 4.546 ; val loss 1.330; lr 1.00e-04]\n",
      "14/300: 100%|████| 168/168 [00:00<00:00, 453.38it/s, train loss 2.676 ; val loss 1.285; lr 1.00e-04]\n",
      "15/300: 100%|████| 168/168 [00:00<00:00, 379.83it/s, train loss 8.480 ; val loss 1.260; lr 1.00e-04]\n",
      "16/300: 100%|████| 168/168 [00:00<00:00, 337.83it/s, train loss 3.714 ; val loss 1.264; lr 1.00e-04]\n",
      "17/300: 100%|████| 168/168 [00:00<00:00, 607.64it/s, train loss 2.285 ; val loss 1.817; lr 1.00e-04]\n",
      "18/300: 100%|████| 168/168 [00:00<00:00, 627.14it/s, train loss 6.293 ; val loss 1.366; lr 1.00e-04]\n",
      "19/300: 100%|████| 168/168 [00:00<00:00, 620.07it/s, train loss 3.553 ; val loss 1.257; lr 1.00e-04]\n",
      "20/300: 100%|████| 168/168 [00:00<00:00, 628.93it/s, train loss 6.196 ; val loss 1.331; lr 1.00e-04]\n",
      "21/300: 100%|████| 168/168 [00:00<00:00, 607.22it/s, train loss 8.176 ; val loss 1.330; lr 1.00e-04]\n",
      "22/300: 100%|████| 168/168 [00:00<00:00, 619.83it/s, train loss 1.493 ; val loss 1.241; lr 1.00e-04]\n",
      "23/300: 100%|███| 168/168 [00:00<00:00, 550.95it/s, train loss 31.403 ; val loss 1.866; lr 1.00e-04]\n",
      "24/300: 100%|████| 168/168 [00:00<00:00, 538.94it/s, train loss 2.060 ; val loss 1.292; lr 1.00e-04]\n",
      "25/300: 100%|████| 168/168 [00:00<00:00, 607.03it/s, train loss 2.643 ; val loss 1.597; lr 1.00e-04]\n",
      "26/300: 100%|████| 168/168 [00:00<00:00, 574.32it/s, train loss 9.191 ; val loss 1.676; lr 1.00e-04]\n",
      "27/300: 100%|████| 168/168 [00:00<00:00, 573.16it/s, train loss 3.160 ; val loss 1.303; lr 1.00e-04]\n",
      "28/300: 100%|████| 168/168 [00:00<00:00, 578.54it/s, train loss 4.083 ; val loss 1.214; lr 1.00e-04]\n",
      "29/300: 100%|████| 168/168 [00:00<00:00, 582.01it/s, train loss 6.752 ; val loss 1.399; lr 1.00e-04]\n",
      "30/300: 100%|████| 168/168 [00:00<00:00, 558.45it/s, train loss 4.526 ; val loss 1.270; lr 1.00e-04]\n",
      "31/300: 100%|████| 168/168 [00:00<00:00, 563.85it/s, train loss 4.907 ; val loss 1.539; lr 1.00e-04]\n",
      "32/300: 100%|████| 168/168 [00:00<00:00, 556.08it/s, train loss 4.593 ; val loss 1.304; lr 1.00e-04]\n",
      "33/300: 100%|████| 168/168 [00:00<00:00, 603.32it/s, train loss 4.365 ; val loss 1.512; lr 1.00e-04]\n",
      "34/300: 100%|████| 168/168 [00:00<00:00, 610.68it/s, train loss 9.388 ; val loss 1.345; lr 1.00e-04]\n",
      "35/300: 100%|████| 168/168 [00:00<00:00, 590.46it/s, train loss 2.912 ; val loss 1.265; lr 1.00e-04]\n",
      "36/300: 100%|████| 168/168 [00:00<00:00, 558.89it/s, train loss 6.296 ; val loss 1.325; lr 1.00e-04]\n",
      "37/300: 100%|████| 168/168 [00:00<00:00, 557.27it/s, train loss 5.289 ; val loss 1.200; lr 1.00e-04]\n",
      "38/300: 100%|████| 168/168 [00:00<00:00, 559.05it/s, train loss 4.346 ; val loss 1.390; lr 1.00e-04]\n",
      "39/300: 100%|████| 168/168 [00:00<00:00, 537.02it/s, train loss 1.516 ; val loss 1.415; lr 1.00e-04]\n",
      "40/300: 100%|████| 168/168 [00:00<00:00, 438.92it/s, train loss 2.362 ; val loss 1.169; lr 1.00e-04]\n",
      "41/300: 100%|████| 168/168 [00:00<00:00, 434.58it/s, train loss 6.859 ; val loss 1.148; lr 1.00e-04]\n",
      "42/300: 100%|████| 168/168 [00:00<00:00, 599.90it/s, train loss 3.537 ; val loss 1.310; lr 1.00e-04]\n",
      "43/300: 100%|████| 168/168 [00:00<00:00, 601.16it/s, train loss 1.791 ; val loss 1.242; lr 1.00e-04]\n",
      "44/300: 100%|████| 168/168 [00:00<00:00, 562.57it/s, train loss 1.229 ; val loss 1.193; lr 1.00e-04]\n",
      "45/300: 100%|████| 168/168 [00:00<00:00, 618.12it/s, train loss 6.138 ; val loss 1.481; lr 1.00e-04]\n",
      "46/300: 100%|████| 168/168 [00:00<00:00, 631.21it/s, train loss 7.584 ; val loss 1.158; lr 1.00e-04]\n",
      "47/300: 100%|███| 168/168 [00:00<00:00, 582.24it/s, train loss 14.282 ; val loss 2.201; lr 1.00e-04]\n",
      "48/300: 100%|████| 168/168 [00:00<00:00, 619.10it/s, train loss 4.367 ; val loss 1.168; lr 1.00e-04]\n",
      "49/300: 100%|████| 168/168 [00:00<00:00, 601.09it/s, train loss 6.401 ; val loss 1.191; lr 1.00e-04]\n",
      "50/300: 100%|████| 168/168 [00:00<00:00, 632.28it/s, train loss 2.248 ; val loss 1.362; lr 1.00e-04]\n",
      "51/300: 100%|████| 168/168 [00:00<00:00, 625.19it/s, train loss 2.213 ; val loss 1.223; lr 1.00e-04]\n",
      "52/300: 100%|████| 168/168 [00:00<00:00, 587.27it/s, train loss 8.117 ; val loss 1.349; lr 1.00e-04]\n",
      "53/300: 100%|████| 168/168 [00:00<00:00, 581.05it/s, train loss 4.631 ; val loss 1.258; lr 1.00e-04]\n",
      "54/300: 100%|███| 168/168 [00:00<00:00, 558.93it/s, train loss 11.234 ; val loss 1.716; lr 1.00e-04]\n",
      "55/300: 100%|████| 168/168 [00:00<00:00, 604.27it/s, train loss 2.963 ; val loss 1.209; lr 1.00e-04]\n",
      "56/300: 100%|████| 168/168 [00:00<00:00, 617.73it/s, train loss 1.104 ; val loss 1.357; lr 1.00e-04]\n",
      "57/300: 100%|███| 168/168 [00:00<00:00, 631.29it/s, train loss 12.359 ; val loss 1.731; lr 1.00e-04]\n",
      "58/300: 100%|████| 168/168 [00:00<00:00, 626.06it/s, train loss 8.156 ; val loss 1.458; lr 1.00e-04]\n",
      "59/300: 100%|████| 168/168 [00:00<00:00, 549.91it/s, train loss 6.516 ; val loss 1.345; lr 1.00e-04]\n",
      "60/300: 100%|████| 168/168 [00:00<00:00, 575.14it/s, train loss 1.919 ; val loss 1.243; lr 1.00e-04]\n",
      "61/300: 100%|████| 168/168 [00:00<00:00, 506.49it/s, train loss 3.232 ; val loss 1.282; lr 1.00e-04]\n",
      "62/300: 100%|████| 168/168 [00:00<00:00, 525.24it/s, train loss 7.123 ; val loss 1.430; lr 1.00e-04]\n",
      "63/300: 100%|████| 168/168 [00:00<00:00, 624.74it/s, train loss 3.651 ; val loss 1.522; lr 1.00e-04]\n",
      "64/300: 100%|████| 168/168 [00:00<00:00, 559.79it/s, train loss 3.528 ; val loss 1.721; lr 1.00e-04]\n",
      "65/300: 100%|████| 168/168 [00:00<00:00, 607.62it/s, train loss 9.128 ; val loss 1.802; lr 1.00e-04]\n",
      "66/300: 100%|████| 168/168 [00:00<00:00, 584.39it/s, train loss 4.329 ; val loss 1.284; lr 1.00e-04]\n",
      "67/300: 100%|████| 168/168 [00:00<00:00, 587.15it/s, train loss 9.163 ; val loss 1.332; lr 1.00e-04]\n",
      "68/300: 100%|████| 168/168 [00:00<00:00, 581.07it/s, train loss 3.045 ; val loss 1.176; lr 1.00e-04]\n",
      "69/300: 100%|████| 168/168 [00:00<00:00, 583.78it/s, train loss 6.403 ; val loss 1.460; lr 1.00e-04]\n",
      "70/300: 100%|████| 168/168 [00:00<00:00, 555.29it/s, train loss 2.181 ; val loss 1.204; lr 1.00e-04]\n",
      "71/300: 100%|████| 168/168 [00:00<00:00, 557.65it/s, train loss 6.340 ; val loss 1.175; lr 1.00e-04]\n",
      "72/300: 100%|████| 168/168 [00:00<00:00, 597.06it/s, train loss 7.433 ; val loss 1.272; lr 1.00e-04]\n",
      "73/300: 100%|████| 168/168 [00:00<00:00, 561.09it/s, train loss 3.757 ; val loss 1.151; lr 1.00e-04]\n",
      "74/300: 100%|████| 168/168 [00:00<00:00, 234.47it/s, train loss 6.389 ; val loss 1.289; lr 1.00e-04]\n",
      "75/300: 100%|███| 168/168 [00:00<00:00, 580.98it/s, train loss 11.770 ; val loss 1.683; lr 1.00e-04]\n",
      "76/300: 100%|████| 168/168 [00:00<00:00, 563.81it/s, train loss 5.489 ; val loss 1.305; lr 1.00e-04]\n",
      "77/300: 100%|████| 168/168 [00:00<00:00, 584.72it/s, train loss 8.275 ; val loss 1.160; lr 1.00e-04]\n",
      "78/300: 100%|████| 168/168 [00:00<00:00, 604.93it/s, train loss 9.425 ; val loss 1.232; lr 1.00e-04]\n",
      "79/300: 100%|████| 168/168 [00:00<00:00, 561.44it/s, train loss 5.612 ; val loss 1.479; lr 1.00e-04]\n",
      "80/300: 100%|████| 168/168 [00:00<00:00, 560.18it/s, train loss 6.147 ; val loss 1.323; lr 1.00e-04]\n",
      "81/300: 100%|███| 168/168 [00:00<00:00, 607.84it/s, train loss 17.120 ; val loss 1.159; lr 1.00e-04]\n",
      "82/300: 100%|████| 168/168 [00:00<00:00, 552.53it/s, train loss 6.415 ; val loss 1.185; lr 1.00e-04]\n",
      "83/300: 100%|████| 168/168 [00:00<00:00, 587.20it/s, train loss 9.337 ; val loss 1.628; lr 1.00e-04]\n",
      "84/300: 100%|███| 168/168 [00:00<00:00, 634.67it/s, train loss 22.428 ; val loss 1.293; lr 1.00e-04]\n",
      "85/300: 100%|████| 168/168 [00:00<00:00, 586.35it/s, train loss 5.978 ; val loss 1.186; lr 1.00e-04]\n",
      "86/300: 100%|████| 168/168 [00:00<00:00, 560.65it/s, train loss 6.102 ; val loss 1.240; lr 1.00e-04]\n",
      "87/300: 100%|████| 168/168 [00:00<00:00, 572.09it/s, train loss 6.502 ; val loss 1.372; lr 1.00e-04]\n",
      "88/300: 100%|████| 168/168 [00:00<00:00, 529.86it/s, train loss 2.321 ; val loss 1.230; lr 1.00e-04]\n",
      "89/300: 100%|████| 168/168 [00:00<00:00, 576.70it/s, train loss 0.736 ; val loss 1.222; lr 1.00e-04]\n",
      "90/300: 100%|███| 168/168 [00:00<00:00, 581.71it/s, train loss 13.389 ; val loss 1.292; lr 1.00e-04]\n",
      "91/300: 100%|████| 168/168 [00:00<00:00, 585.06it/s, train loss 4.574 ; val loss 1.172; lr 1.00e-04]\n",
      "92/300: 100%|████| 168/168 [00:00<00:00, 592.89it/s, train loss 4.607 ; val loss 1.208; lr 1.00e-04]\n",
      "93/300: 100%|███| 168/168 [00:00<00:00, 533.33it/s, train loss 10.681 ; val loss 1.491; lr 1.00e-04]\n",
      "94/300: 100%|████| 168/168 [00:00<00:00, 534.09it/s, train loss 3.654 ; val loss 1.545; lr 1.00e-04]\n",
      "95/300: 100%|████| 168/168 [00:00<00:00, 476.93it/s, train loss 2.347 ; val loss 1.635; lr 1.00e-04]\n",
      "96/300: 100%|████| 168/168 [00:00<00:00, 593.26it/s, train loss 5.120 ; val loss 1.384; lr 1.00e-04]\n",
      "97/300: 100%|███| 168/168 [00:00<00:00, 584.21it/s, train loss 15.683 ; val loss 1.153; lr 1.00e-04]\n",
      "98/300: 100%|████| 168/168 [00:00<00:00, 570.46it/s, train loss 8.490 ; val loss 1.642; lr 1.00e-04]\n",
      "99/300: 100%|████| 168/168 [00:00<00:00, 570.94it/s, train loss 3.135 ; val loss 1.198; lr 1.00e-04]\n",
      "100/300: 100%|███| 168/168 [00:00<00:00, 597.73it/s, train loss 5.657 ; val loss 1.266; lr 1.00e-04]\n",
      "101/300: 100%|███| 168/168 [00:00<00:00, 567.14it/s, train loss 3.392 ; val loss 1.213; lr 1.00e-05]\n",
      "102/300: 100%|███| 168/168 [00:00<00:00, 610.41it/s, train loss 5.308 ; val loss 1.256; lr 1.00e-05]\n",
      "103/300: 100%|███| 168/168 [00:00<00:00, 625.38it/s, train loss 1.514 ; val loss 1.186; lr 1.00e-05]\n",
      "104/300: 100%|███| 168/168 [00:00<00:00, 542.97it/s, train loss 4.706 ; val loss 1.428; lr 1.00e-05]\n",
      "105/300: 100%|███| 168/168 [00:00<00:00, 612.42it/s, train loss 0.891 ; val loss 1.419; lr 1.00e-05]\n",
      "106/300: 100%|███| 168/168 [00:00<00:00, 635.14it/s, train loss 3.297 ; val loss 1.435; lr 1.00e-05]\n",
      "107/300: 100%|███| 168/168 [00:00<00:00, 623.82it/s, train loss 2.093 ; val loss 1.197; lr 1.00e-05]\n",
      "108/300: 100%|███| 168/168 [00:00<00:00, 613.34it/s, train loss 3.234 ; val loss 1.244; lr 1.00e-05]\n",
      "109/300: 100%|███| 168/168 [00:00<00:00, 602.89it/s, train loss 2.475 ; val loss 1.385; lr 1.00e-05]\n",
      "110/300: 100%|███| 168/168 [00:00<00:00, 521.71it/s, train loss 8.258 ; val loss 1.255; lr 1.00e-05]\n",
      "111/300: 100%|███| 168/168 [00:00<00:00, 444.83it/s, train loss 1.994 ; val loss 1.205; lr 1.00e-05]\n",
      "112/300: 100%|███| 168/168 [00:00<00:00, 513.17it/s, train loss 1.219 ; val loss 1.150; lr 1.00e-05]\n",
      "113/300: 100%|███| 168/168 [00:00<00:00, 202.91it/s, train loss 1.872 ; val loss 1.223; lr 1.00e-05]\n",
      "114/300: 100%|███| 168/168 [00:00<00:00, 423.08it/s, train loss 1.404 ; val loss 1.151; lr 1.00e-05]\n",
      "115/300: 100%|███| 168/168 [00:00<00:00, 421.11it/s, train loss 7.449 ; val loss 1.453; lr 1.00e-05]\n",
      "116/300: 100%|███| 168/168 [00:00<00:00, 573.06it/s, train loss 3.957 ; val loss 1.947; lr 1.00e-05]\n",
      "117/300: 100%|███| 168/168 [00:00<00:00, 632.87it/s, train loss 6.469 ; val loss 1.196; lr 1.00e-05]\n",
      "118/300: 100%|██| 168/168 [00:00<00:00, 620.59it/s, train loss 18.815 ; val loss 1.275; lr 1.00e-05]\n",
      "119/300: 100%|███| 168/168 [00:00<00:00, 622.48it/s, train loss 2.204 ; val loss 1.184; lr 1.00e-05]\n",
      "120/300: 100%|██| 168/168 [00:00<00:00, 632.68it/s, train loss 15.469 ; val loss 1.243; lr 1.00e-05]\n",
      "121/300: 100%|███| 168/168 [00:00<00:00, 628.31it/s, train loss 7.427 ; val loss 1.281; lr 1.00e-05]\n",
      "122/300: 100%|███| 168/168 [00:00<00:00, 433.73it/s, train loss 5.371 ; val loss 1.211; lr 1.00e-05]\n",
      "123/300: 100%|██| 168/168 [00:00<00:00, 585.89it/s, train loss 11.439 ; val loss 1.891; lr 1.00e-05]\n",
      "124/300: 100%|███| 168/168 [00:00<00:00, 509.10it/s, train loss 3.032 ; val loss 1.229; lr 1.00e-05]\n",
      "125/300: 100%|███| 168/168 [00:00<00:00, 429.55it/s, train loss 9.656 ; val loss 1.766; lr 1.00e-05]\n",
      "126/300: 100%|███| 168/168 [00:00<00:00, 423.11it/s, train loss 1.955 ; val loss 1.263; lr 1.00e-05]\n",
      "127/300: 100%|███| 168/168 [00:00<00:00, 342.99it/s, train loss 1.679 ; val loss 1.397; lr 1.00e-05]\n",
      "128/300: 100%|███| 168/168 [00:00<00:00, 522.74it/s, train loss 5.036 ; val loss 1.232; lr 1.00e-05]\n",
      "129/300: 100%|███| 168/168 [00:00<00:00, 568.56it/s, train loss 2.433 ; val loss 1.228; lr 1.00e-05]\n",
      "130/300: 100%|███| 168/168 [00:00<00:00, 587.67it/s, train loss 2.493 ; val loss 1.167; lr 1.00e-05]\n",
      "131/300: 100%|███| 168/168 [00:00<00:00, 531.67it/s, train loss 1.851 ; val loss 1.210; lr 1.00e-05]\n",
      "132/300: 100%|███| 168/168 [00:00<00:00, 587.36it/s, train loss 3.208 ; val loss 1.805; lr 1.00e-05]\n",
      "133/300: 100%|██| 168/168 [00:00<00:00, 594.18it/s, train loss 11.757 ; val loss 1.396; lr 1.00e-05]\n",
      "134/300: 100%|███| 168/168 [00:00<00:00, 598.68it/s, train loss 7.472 ; val loss 1.164; lr 1.00e-05]\n",
      "135/300: 100%|██| 168/168 [00:00<00:00, 614.80it/s, train loss 12.584 ; val loss 1.163; lr 1.00e-05]\n",
      "136/300: 100%|███| 168/168 [00:00<00:00, 603.11it/s, train loss 2.895 ; val loss 1.422; lr 1.00e-05]\n",
      "137/300: 100%|███| 168/168 [00:00<00:00, 611.73it/s, train loss 3.356 ; val loss 1.555; lr 1.00e-05]\n",
      "138/300: 100%|███| 168/168 [00:00<00:00, 554.69it/s, train loss 2.090 ; val loss 1.193; lr 1.00e-05]\n",
      "139/300: 100%|███| 168/168 [00:00<00:00, 569.35it/s, train loss 5.253 ; val loss 1.281; lr 1.00e-05]\n",
      "140/300: 100%|███| 168/168 [00:00<00:00, 596.41it/s, train loss 3.343 ; val loss 1.427; lr 1.00e-05]\n",
      "141/300: 100%|███| 168/168 [00:00<00:00, 571.91it/s, train loss 2.001 ; val loss 1.188; lr 1.00e-05]\n",
      "142/300: 100%|███| 168/168 [00:00<00:00, 615.57it/s, train loss 8.936 ; val loss 1.215; lr 1.00e-05]\n",
      "143/300: 100%|███| 168/168 [00:00<00:00, 626.12it/s, train loss 7.553 ; val loss 1.354; lr 1.00e-05]\n",
      "144/300: 100%|███| 168/168 [00:00<00:00, 624.28it/s, train loss 5.087 ; val loss 1.721; lr 1.00e-05]\n",
      "145/300: 100%|███| 168/168 [00:00<00:00, 598.37it/s, train loss 8.029 ; val loss 1.175; lr 1.00e-05]\n",
      "146/300: 100%|███| 168/168 [00:00<00:00, 624.31it/s, train loss 4.615 ; val loss 1.189; lr 1.00e-05]\n",
      "147/300: 100%|██| 168/168 [00:00<00:00, 620.60it/s, train loss 12.582 ; val loss 1.163; lr 1.00e-05]\n",
      "148/300: 100%|███| 168/168 [00:00<00:00, 617.15it/s, train loss 5.413 ; val loss 1.157; lr 1.00e-05]\n",
      "149/300: 100%|███| 168/168 [00:00<00:00, 627.13it/s, train loss 8.724 ; val loss 1.245; lr 1.00e-05]\n",
      "150/300: 100%|███| 168/168 [00:00<00:00, 586.04it/s, train loss 1.832 ; val loss 1.169; lr 1.00e-05]\n",
      "151/300: 100%|███| 168/168 [00:00<00:00, 532.52it/s, train loss 1.907 ; val loss 1.316; lr 1.00e-05]\n",
      "152/300: 100%|███| 168/168 [00:00<00:00, 500.12it/s, train loss 1.691 ; val loss 1.269; lr 1.00e-05]\n",
      "153/300: 100%|███| 168/168 [00:00<00:00, 589.56it/s, train loss 1.067 ; val loss 1.257; lr 1.00e-05]\n",
      "154/300: 100%|██| 168/168 [00:00<00:00, 550.35it/s, train loss 14.486 ; val loss 1.641; lr 1.00e-05]\n",
      "155/300: 100%|███| 168/168 [00:00<00:00, 496.96it/s, train loss 4.145 ; val loss 1.219; lr 1.00e-05]\n",
      "156/300: 100%|███| 168/168 [00:00<00:00, 621.80it/s, train loss 1.794 ; val loss 1.163; lr 1.00e-05]\n",
      "157/300: 100%|███| 168/168 [00:00<00:00, 542.78it/s, train loss 3.026 ; val loss 1.272; lr 1.00e-05]\n",
      "158/300: 100%|███| 168/168 [00:00<00:00, 487.09it/s, train loss 9.634 ; val loss 1.225; lr 1.00e-05]\n",
      "159/300: 100%|███| 168/168 [00:00<00:00, 570.40it/s, train loss 3.222 ; val loss 1.203; lr 1.00e-05]\n",
      "160/300: 100%|██| 168/168 [00:00<00:00, 612.73it/s, train loss 20.095 ; val loss 1.332; lr 1.00e-05]\n",
      "161/300: 100%|███| 168/168 [00:00<00:00, 612.78it/s, train loss 6.654 ; val loss 1.545; lr 1.00e-05]\n",
      "162/300: 100%|███| 168/168 [00:00<00:00, 633.06it/s, train loss 3.438 ; val loss 1.323; lr 1.00e-05]\n",
      "163/300: 100%|███| 168/168 [00:00<00:00, 560.30it/s, train loss 3.942 ; val loss 1.196; lr 1.00e-05]\n",
      "164/300: 100%|███| 168/168 [00:00<00:00, 621.64it/s, train loss 2.841 ; val loss 1.663; lr 1.00e-05]\n",
      "165/300: 100%|███| 168/168 [00:00<00:00, 580.13it/s, train loss 0.487 ; val loss 1.213; lr 1.00e-05]\n",
      "166/300: 100%|███| 168/168 [00:00<00:00, 565.74it/s, train loss 2.262 ; val loss 1.253; lr 1.00e-05]\n",
      "167/300: 100%|██| 168/168 [00:00<00:00, 599.72it/s, train loss 16.302 ; val loss 2.039; lr 1.00e-05]\n",
      "168/300: 100%|███| 168/168 [00:00<00:00, 633.10it/s, train loss 7.362 ; val loss 1.278; lr 1.00e-05]\n",
      "169/300: 100%|███| 168/168 [00:00<00:00, 625.28it/s, train loss 8.694 ; val loss 1.563; lr 1.00e-05]\n",
      "170/300: 100%|███| 168/168 [00:00<00:00, 577.46it/s, train loss 0.563 ; val loss 1.156; lr 1.00e-05]\n",
      "171/300: 100%|██| 168/168 [00:00<00:00, 634.99it/s, train loss 18.221 ; val loss 1.397; lr 1.00e-05]\n",
      "172/300: 100%|███| 168/168 [00:00<00:00, 624.17it/s, train loss 3.723 ; val loss 1.147; lr 1.00e-05]\n",
      "173/300: 100%|███| 168/168 [00:00<00:00, 627.08it/s, train loss 5.451 ; val loss 1.150; lr 1.00e-05]\n",
      "174/300: 100%|███| 168/168 [00:00<00:00, 579.95it/s, train loss 6.069 ; val loss 1.146; lr 1.00e-05]\n",
      "175/300: 100%|███| 168/168 [00:00<00:00, 574.51it/s, train loss 2.935 ; val loss 1.197; lr 1.00e-05]\n",
      "176/300: 100%|███| 168/168 [00:00<00:00, 569.51it/s, train loss 2.510 ; val loss 1.282; lr 1.00e-05]\n",
      "177/300: 100%|███| 168/168 [00:00<00:00, 572.59it/s, train loss 2.500 ; val loss 1.412; lr 1.00e-05]\n",
      "178/300: 100%|███| 168/168 [00:00<00:00, 595.57it/s, train loss 2.012 ; val loss 1.461; lr 1.00e-05]\n",
      "179/300: 100%|███| 168/168 [00:00<00:00, 582.92it/s, train loss 1.682 ; val loss 1.166; lr 1.00e-05]\n",
      "180/300: 100%|██| 168/168 [00:00<00:00, 551.88it/s, train loss 10.315 ; val loss 1.442; lr 1.00e-05]\n",
      "181/300: 100%|███| 168/168 [00:00<00:00, 619.91it/s, train loss 1.822 ; val loss 1.236; lr 1.00e-05]\n",
      "182/300: 100%|███| 168/168 [00:00<00:00, 597.35it/s, train loss 2.601 ; val loss 1.411; lr 1.00e-05]\n",
      "183/300: 100%|███| 168/168 [00:00<00:00, 623.76it/s, train loss 2.542 ; val loss 1.399; lr 1.00e-05]\n",
      "184/300: 100%|███| 168/168 [00:00<00:00, 446.82it/s, train loss 4.943 ; val loss 1.138; lr 1.00e-05]\n",
      "185/300: 100%|███| 168/168 [00:00<00:00, 594.07it/s, train loss 5.706 ; val loss 1.214; lr 1.00e-05]\n",
      "186/300: 100%|██| 168/168 [00:00<00:00, 453.50it/s, train loss 15.677 ; val loss 1.240; lr 1.00e-05]\n",
      "187/300: 100%|███| 168/168 [00:00<00:00, 474.32it/s, train loss 2.046 ; val loss 1.152; lr 1.00e-05]\n",
      "188/300: 100%|███| 168/168 [00:00<00:00, 540.93it/s, train loss 9.510 ; val loss 1.771; lr 1.00e-05]\n",
      "189/300: 100%|███| 168/168 [00:00<00:00, 609.47it/s, train loss 1.666 ; val loss 1.180; lr 1.00e-05]\n",
      "190/300: 100%|███| 168/168 [00:00<00:00, 229.41it/s, train loss 3.464 ; val loss 1.354; lr 1.00e-05]\n",
      "191/300: 100%|███| 168/168 [00:00<00:00, 543.33it/s, train loss 2.534 ; val loss 1.625; lr 1.00e-05]\n",
      "192/300: 100%|███| 168/168 [00:00<00:00, 568.30it/s, train loss 4.928 ; val loss 1.197; lr 1.00e-05]\n",
      "193/300: 100%|██| 168/168 [00:00<00:00, 561.05it/s, train loss 23.216 ; val loss 1.514; lr 1.00e-05]\n",
      "194/300: 100%|██| 168/168 [00:00<00:00, 569.02it/s, train loss 31.673 ; val loss 1.241; lr 1.00e-05]\n",
      "195/300: 100%|███| 168/168 [00:00<00:00, 607.61it/s, train loss 6.504 ; val loss 1.253; lr 1.00e-05]\n",
      "196/300: 100%|███| 168/168 [00:00<00:00, 580.79it/s, train loss 6.270 ; val loss 1.565; lr 1.00e-05]\n",
      "197/300: 100%|██| 168/168 [00:00<00:00, 494.45it/s, train loss 14.125 ; val loss 1.282; lr 1.00e-05]\n",
      "198/300: 100%|███| 168/168 [00:00<00:00, 579.60it/s, train loss 6.286 ; val loss 1.151; lr 1.00e-05]\n",
      "199/300: 100%|███| 168/168 [00:00<00:00, 601.87it/s, train loss 1.725 ; val loss 1.169; lr 1.00e-05]\n",
      "200/300: 100%|███| 168/168 [00:00<00:00, 584.88it/s, train loss 2.489 ; val loss 1.249; lr 1.00e-05]\n",
      "201/300: 100%|██| 168/168 [00:00<00:00, 596.74it/s, train loss 25.013 ; val loss 1.793; lr 1.00e-06]\n",
      "202/300: 100%|███| 168/168 [00:00<00:00, 604.17it/s, train loss 2.700 ; val loss 1.292; lr 1.00e-06]\n",
      "203/300: 100%|███| 168/168 [00:00<00:00, 618.59it/s, train loss 8.220 ; val loss 1.418; lr 1.00e-06]\n",
      "204/300: 100%|███| 168/168 [00:00<00:00, 570.44it/s, train loss 5.597 ; val loss 1.172; lr 1.00e-06]\n",
      "205/300: 100%|██| 168/168 [00:00<00:00, 580.06it/s, train loss 12.234 ; val loss 1.284; lr 1.00e-06]\n",
      "206/300: 100%|███| 168/168 [00:00<00:00, 572.86it/s, train loss 6.754 ; val loss 1.643; lr 1.00e-06]\n",
      "207/300: 100%|███| 168/168 [00:00<00:00, 483.83it/s, train loss 2.402 ; val loss 1.230; lr 1.00e-06]\n",
      "208/300: 100%|███| 168/168 [00:00<00:00, 536.20it/s, train loss 8.815 ; val loss 1.504; lr 1.00e-06]\n",
      "209/300: 100%|██| 168/168 [00:00<00:00, 521.76it/s, train loss 12.218 ; val loss 1.462; lr 1.00e-06]\n",
      "210/300: 100%|███| 168/168 [00:00<00:00, 572.70it/s, train loss 1.653 ; val loss 1.570; lr 1.00e-06]\n",
      "211/300: 100%|███| 168/168 [00:00<00:00, 505.59it/s, train loss 4.525 ; val loss 1.170; lr 1.00e-06]\n",
      "212/300: 100%|███| 168/168 [00:00<00:00, 603.71it/s, train loss 1.703 ; val loss 2.084; lr 1.00e-06]\n",
      "213/300: 100%|██| 168/168 [00:00<00:00, 625.17it/s, train loss 17.154 ; val loss 1.344; lr 1.00e-06]\n",
      "214/300: 100%|███| 168/168 [00:00<00:00, 623.02it/s, train loss 3.448 ; val loss 1.259; lr 1.00e-06]\n",
      "215/300: 100%|███| 168/168 [00:00<00:00, 572.53it/s, train loss 2.205 ; val loss 1.344; lr 1.00e-06]\n",
      "216/300: 100%|███| 168/168 [00:00<00:00, 469.57it/s, train loss 1.526 ; val loss 1.660; lr 1.00e-06]\n",
      "217/300: 100%|███| 168/168 [00:00<00:00, 563.99it/s, train loss 1.441 ; val loss 1.169; lr 1.00e-06]\n",
      "218/300: 100%|███| 168/168 [00:00<00:00, 549.12it/s, train loss 2.261 ; val loss 1.155; lr 1.00e-06]\n",
      "219/300: 100%|███| 168/168 [00:00<00:00, 541.51it/s, train loss 1.477 ; val loss 1.179; lr 1.00e-06]\n",
      "220/300: 100%|██| 168/168 [00:00<00:00, 529.77it/s, train loss 13.795 ; val loss 1.229; lr 1.00e-06]\n",
      "221/300: 100%|███| 168/168 [00:00<00:00, 551.29it/s, train loss 8.970 ; val loss 1.578; lr 1.00e-06]\n",
      "222/300: 100%|███| 168/168 [00:00<00:00, 548.88it/s, train loss 4.860 ; val loss 1.232; lr 1.00e-06]\n",
      "223/300: 100%|███| 168/168 [00:00<00:00, 594.09it/s, train loss 1.718 ; val loss 1.634; lr 1.00e-06]\n",
      "224/300: 100%|███| 168/168 [00:00<00:00, 502.52it/s, train loss 2.203 ; val loss 1.392; lr 1.00e-06]\n",
      "225/300: 100%|███| 168/168 [00:00<00:00, 515.08it/s, train loss 4.345 ; val loss 1.496; lr 1.00e-06]\n",
      "226/300: 100%|███| 168/168 [00:00<00:00, 470.32it/s, train loss 9.475 ; val loss 1.146; lr 1.00e-06]\n",
      "227/300: 100%|███| 168/168 [00:00<00:00, 479.37it/s, train loss 1.287 ; val loss 1.157; lr 1.00e-06]\n",
      "228/300: 100%|███| 168/168 [00:00<00:00, 532.97it/s, train loss 5.337 ; val loss 1.345; lr 1.00e-06]\n",
      "229/300: 100%|███| 168/168 [00:00<00:00, 508.27it/s, train loss 2.236 ; val loss 1.300; lr 1.00e-06]\n",
      "230/300: 100%|███| 168/168 [00:00<00:00, 495.72it/s, train loss 4.110 ; val loss 1.731; lr 1.00e-06]\n",
      "231/300: 100%|███| 168/168 [00:00<00:00, 593.28it/s, train loss 3.020 ; val loss 1.247; lr 1.00e-06]\n",
      "232/300: 100%|██| 168/168 [00:00<00:00, 382.01it/s, train loss 25.496 ; val loss 1.608; lr 1.00e-06]\n",
      "233/300: 100%|███| 168/168 [00:00<00:00, 435.82it/s, train loss 4.685 ; val loss 1.740; lr 1.00e-06]\n",
      "234/300: 100%|██| 168/168 [00:00<00:00, 440.91it/s, train loss 14.920 ; val loss 2.389; lr 1.00e-06]\n",
      "235/300: 100%|███| 168/168 [00:00<00:00, 505.72it/s, train loss 2.634 ; val loss 1.306; lr 1.00e-06]\n",
      "236/300: 100%|███| 168/168 [00:00<00:00, 600.81it/s, train loss 4.508 ; val loss 1.232; lr 1.00e-06]\n",
      "237/300: 100%|███| 168/168 [00:00<00:00, 557.13it/s, train loss 3.117 ; val loss 1.267; lr 1.00e-06]\n",
      "238/300: 100%|███| 168/168 [00:00<00:00, 454.23it/s, train loss 8.948 ; val loss 1.545; lr 1.00e-06]\n",
      "239/300: 100%|███| 168/168 [00:00<00:00, 559.47it/s, train loss 2.876 ; val loss 1.299; lr 1.00e-06]\n",
      "240/300: 100%|███| 168/168 [00:00<00:00, 542.31it/s, train loss 4.058 ; val loss 1.216; lr 1.00e-06]\n",
      "241/300: 100%|███| 168/168 [00:00<00:00, 613.86it/s, train loss 1.873 ; val loss 1.672; lr 1.00e-06]\n",
      "242/300: 100%|███| 168/168 [00:00<00:00, 523.82it/s, train loss 1.797 ; val loss 1.209; lr 1.00e-06]\n",
      "243/300: 100%|███| 168/168 [00:00<00:00, 388.63it/s, train loss 6.483 ; val loss 1.460; lr 1.00e-06]\n",
      "244/300: 100%|██| 168/168 [00:00<00:00, 464.02it/s, train loss 11.625 ; val loss 1.895; lr 1.00e-06]\n",
      "245/300: 100%|███| 168/168 [00:00<00:00, 434.41it/s, train loss 4.900 ; val loss 1.142; lr 1.00e-06]\n",
      "246/300: 100%|███| 168/168 [00:00<00:00, 596.32it/s, train loss 3.542 ; val loss 1.368; lr 1.00e-06]\n",
      "247/300: 100%|███| 168/168 [00:00<00:00, 573.85it/s, train loss 2.938 ; val loss 1.209; lr 1.00e-06]\n",
      "248/300: 100%|███| 168/168 [00:00<00:00, 573.32it/s, train loss 8.870 ; val loss 1.273; lr 1.00e-06]\n",
      "249/300: 100%|███| 168/168 [00:00<00:00, 534.51it/s, train loss 1.160 ; val loss 1.347; lr 1.00e-06]\n",
      "250/300: 100%|███| 168/168 [00:00<00:00, 557.87it/s, train loss 4.968 ; val loss 2.120; lr 1.00e-06]\n",
      "251/300: 100%|███| 168/168 [00:00<00:00, 552.71it/s, train loss 3.607 ; val loss 1.278; lr 1.00e-06]\n",
      "252/300: 100%|███| 168/168 [00:00<00:00, 521.76it/s, train loss 3.030 ; val loss 1.210; lr 1.00e-06]\n",
      "253/300: 100%|███| 168/168 [00:00<00:00, 578.75it/s, train loss 6.987 ; val loss 1.622; lr 1.00e-06]\n",
      "254/300: 100%|███| 168/168 [00:00<00:00, 598.97it/s, train loss 7.266 ; val loss 1.156; lr 1.00e-06]\n",
      "255/300: 100%|███| 168/168 [00:00<00:00, 613.55it/s, train loss 3.969 ; val loss 1.219; lr 1.00e-06]\n",
      "256/300: 100%|███| 168/168 [00:00<00:00, 577.33it/s, train loss 1.345 ; val loss 1.522; lr 1.00e-06]\n",
      "257/300: 100%|███| 168/168 [00:00<00:00, 618.27it/s, train loss 2.992 ; val loss 1.265; lr 1.00e-06]\n",
      "258/300: 100%|███| 168/168 [00:00<00:00, 565.27it/s, train loss 1.681 ; val loss 1.247; lr 1.00e-06]\n",
      "259/300: 100%|███| 168/168 [00:00<00:00, 583.65it/s, train loss 7.251 ; val loss 1.438; lr 1.00e-06]\n",
      "260/300: 100%|███| 168/168 [00:00<00:00, 600.58it/s, train loss 2.434 ; val loss 1.285; lr 1.00e-06]\n",
      "261/300: 100%|███| 168/168 [00:00<00:00, 584.25it/s, train loss 3.023 ; val loss 1.455; lr 1.00e-06]\n",
      "262/300: 100%|███| 168/168 [00:00<00:00, 568.99it/s, train loss 3.518 ; val loss 1.243; lr 1.00e-06]\n",
      "263/300: 100%|███| 168/168 [00:00<00:00, 568.40it/s, train loss 3.115 ; val loss 1.437; lr 1.00e-06]\n",
      "264/300: 100%|███| 168/168 [00:00<00:00, 570.55it/s, train loss 3.452 ; val loss 1.221; lr 1.00e-06]\n",
      "265/300: 100%|███| 168/168 [00:00<00:00, 626.86it/s, train loss 4.744 ; val loss 1.258; lr 1.00e-06]\n",
      "266/300: 100%|███| 168/168 [00:00<00:00, 629.24it/s, train loss 7.631 ; val loss 1.207; lr 1.00e-06]\n",
      "267/300: 100%|███| 168/168 [00:00<00:00, 628.53it/s, train loss 8.883 ; val loss 1.332; lr 1.00e-06]\n",
      "268/300: 100%|██| 168/168 [00:00<00:00, 615.36it/s, train loss 10.140 ; val loss 2.275; lr 1.00e-06]\n",
      "269/300: 100%|███| 168/168 [00:00<00:00, 632.80it/s, train loss 5.472 ; val loss 1.526; lr 1.00e-06]\n",
      "270/300: 100%|███| 168/168 [00:00<00:00, 624.69it/s, train loss 2.656 ; val loss 1.233; lr 1.00e-06]\n",
      "271/300: 100%|███| 168/168 [00:00<00:00, 641.93it/s, train loss 3.494 ; val loss 1.167; lr 1.00e-06]\n",
      "272/300: 100%|███| 168/168 [00:00<00:00, 627.66it/s, train loss 2.311 ; val loss 1.511; lr 1.00e-06]\n",
      "273/300: 100%|██| 168/168 [00:00<00:00, 564.37it/s, train loss 18.325 ; val loss 1.234; lr 1.00e-06]\n",
      "274/300: 100%|███| 168/168 [00:00<00:00, 630.09it/s, train loss 4.550 ; val loss 1.281; lr 1.00e-06]\n",
      "275/300: 100%|███| 168/168 [00:00<00:00, 635.95it/s, train loss 3.356 ; val loss 1.246; lr 1.00e-06]\n",
      "276/300: 100%|███| 168/168 [00:00<00:00, 599.45it/s, train loss 1.714 ; val loss 1.149; lr 1.00e-06]\n",
      "277/300: 100%|███| 168/168 [00:00<00:00, 603.02it/s, train loss 6.737 ; val loss 1.337; lr 1.00e-06]\n",
      "278/300: 100%|███| 168/168 [00:00<00:00, 235.11it/s, train loss 1.522 ; val loss 1.432; lr 1.00e-06]\n",
      "279/300: 100%|██| 168/168 [00:00<00:00, 555.22it/s, train loss 10.720 ; val loss 1.386; lr 1.00e-06]\n",
      "280/300: 100%|███| 168/168 [00:00<00:00, 590.47it/s, train loss 5.033 ; val loss 1.422; lr 1.00e-06]\n",
      "281/300: 100%|███| 168/168 [00:00<00:00, 616.69it/s, train loss 3.351 ; val loss 1.632; lr 1.00e-06]\n",
      "282/300: 100%|██| 168/168 [00:00<00:00, 522.77it/s, train loss 11.688 ; val loss 1.199; lr 1.00e-06]\n",
      "283/300: 100%|███| 168/168 [00:00<00:00, 493.85it/s, train loss 0.778 ; val loss 1.161; lr 1.00e-06]\n",
      "284/300: 100%|███| 168/168 [00:00<00:00, 602.49it/s, train loss 2.540 ; val loss 1.185; lr 1.00e-06]\n",
      "285/300: 100%|███| 168/168 [00:00<00:00, 609.82it/s, train loss 2.026 ; val loss 1.213; lr 1.00e-06]\n",
      "286/300: 100%|██| 168/168 [00:00<00:00, 584.44it/s, train loss 14.446 ; val loss 1.489; lr 1.00e-06]\n",
      "287/300: 100%|███| 168/168 [00:00<00:00, 549.91it/s, train loss 0.842 ; val loss 1.282; lr 1.00e-06]\n",
      "288/300: 100%|██| 168/168 [00:00<00:00, 593.61it/s, train loss 10.548 ; val loss 1.373; lr 1.00e-06]\n",
      "289/300: 100%|███| 168/168 [00:00<00:00, 552.27it/s, train loss 3.946 ; val loss 1.333; lr 1.00e-06]\n",
      "290/300: 100%|███| 168/168 [00:00<00:00, 561.67it/s, train loss 6.768 ; val loss 1.356; lr 1.00e-06]\n",
      "291/300: 100%|███| 168/168 [00:00<00:00, 476.46it/s, train loss 1.564 ; val loss 1.239; lr 1.00e-06]\n",
      "292/300: 100%|███| 168/168 [00:00<00:00, 452.83it/s, train loss 4.301 ; val loss 1.585; lr 1.00e-06]\n",
      "293/300: 100%|███| 168/168 [00:00<00:00, 468.59it/s, train loss 2.783 ; val loss 1.450; lr 1.00e-06]\n",
      "294/300: 100%|███| 168/168 [00:00<00:00, 431.58it/s, train loss 1.593 ; val loss 1.273; lr 1.00e-06]\n",
      "295/300: 100%|███| 168/168 [00:00<00:00, 592.43it/s, train loss 7.286 ; val loss 1.157; lr 1.00e-06]\n",
      "296/300: 100%|███| 168/168 [00:00<00:00, 554.85it/s, train loss 3.900 ; val loss 1.156; lr 1.00e-06]\n",
      "297/300: 100%|███| 168/168 [00:00<00:00, 633.89it/s, train loss 8.708 ; val loss 1.939; lr 1.00e-06]\n",
      "298/300: 100%|███| 168/168 [00:00<00:00, 612.58it/s, train loss 5.538 ; val loss 1.315; lr 1.00e-06]\n",
      "299/300: 100%|███| 168/168 [00:00<00:00, 585.33it/s, train loss 4.309 ; val loss 1.249; lr 1.00e-06]\n",
      "300/300: 100%|███| 168/168 [00:00<00:00, 632.38it/s, train loss 6.694 ; val loss 1.190; lr 1.00e-06]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(COVID19Dataset(x_dataset.values, y_dataset.values),   # 使用全部的训练数据\n",
    "                            batch_size=16, \n",
    "                            shuffle=True, \n",
    "                            num_workers=0, \n",
    "                            drop_last=True)\n",
    "\n",
    "val_loader = DataLoader(COVID19Dataset(x_val.values, y_val.values),   # 从训练数据中拿出一部分测试（其实这部分数据也参加了训练）\n",
    "                        batch_size=8, \n",
    "                        shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Model(input_channel=len(selected_feature_idx[1:])).to(device=device)\n",
    "loss_fcn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200], gamma=0.1)\n",
    "\n",
    "total_epoch = 300\n",
    "for epoch in range(total_epoch):\n",
    "    with tqdm(train_loader, total=len(train_loader), ncols=100) as tbar:\n",
    "        tbar.set_description(f\"{epoch+1}/{total_epoch}\")\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # print(x, y)\n",
    "            model.train()\n",
    "            x = x.float().to(device)\n",
    "            preds = model(x).squeeze(dim=1)                \n",
    "            loss = loss_fcn(y.float().to(device), preds)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                tot_mse = []\n",
    "                for j, (x, y) in enumerate(val_loader):\n",
    "                    model.eval()\n",
    "                    preds = model(x.float().to(device)).squeeze(dim=1)\n",
    "                    tot_mse.append(loss_fcn(y.float().to(device), preds).detach().cpu().numpy())\n",
    "                tbar.set_postfix_str(f'train loss {loss.item():.3f} ; val loss {np.mean(tot_mse):.3f}; lr {lr_scheduler.get_last_lr()[0]:.2e}')\n",
    "            tbar.update(1)\n",
    "            \n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j, (x, y) in enumerate(val_loader):\n",
    "#     model.eval()\n",
    "#     preds = model(x.float().to(device)).squeeze(dim=1)\n",
    "#     print(preds, y, sep='\\n')\n",
    "#     print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(test_data_path)\n",
    "test_dataset = test_dataset[test_dataset.columns[1:]]\n",
    "test_dataset = feature_process_pipeline.transform(test_dataset.copy())\n",
    "\n",
    "test_dataset = test_dataset[selected_feature_idx[1:]]\n",
    "test_loader = DataLoader(COVID19Dataset(test_dataset.values, None), batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "preds_all = []\n",
    "for x in test_loader:\n",
    "    x = x.float().to(device)\n",
    "    preds = model(x).detach().cpu().numpy().squeeze()\n",
    "    # print(preds)\n",
    "    preds_all.extend(preds)\n",
    "\n",
    "save_pred(preds_all, \"./dl_selected_maxmin_normalized_feature.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de928e550a88750d4b4eaac032b56746d48a43a47ab39c53bff65f93a9980f92"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
