{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport math\nimport torch\nimport numpy as np\nfrom random import *\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as Data","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:34.031155Z","iopub.execute_input":"2022-03-29T08:23:34.031767Z","iopub.status.idle":"2022-03-29T08:23:35.545083Z","shell.execute_reply.started":"2022-03-29T08:23:34.031663Z","shell.execute_reply":"2022-03-29T08:23:35.544395Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ntext = (\n    'Hello, how are you? I am Romeo.\\n' # R\n    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n    'Nice meet you too. How are you today?\\n' # R\n    'Great. My baseball team won the competition.\\n' # J\n    'Oh Congratulations, Juliet\\n' # R\n    'Thank you Romeo\\n' # J\n    'Where are you going today?\\n' # R\n    'I am going shopping. What about you?\\n' # J\n    'I am going to visit my grandmother. she is not very well' # R\n)\nsentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!'\nword_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\nword2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\nfor i, w in enumerate(word_list):\n    word2idx[w] = i + 4\nidx2word = {i: w for i, w in enumerate(word2idx)}\nvocab_size = len(word2idx)\n\ntoken_list = list()\nfor sentence in sentences:\n    arr = [word2idx[s] for s in sentence.split()]\n    token_list.append(arr)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.546620Z","iopub.execute_input":"2022-03-29T08:23:35.546866Z","iopub.status.idle":"2022-03-29T08:23:35.556683Z","shell.execute_reply.started":"2022-03-29T08:23:35.546832Z","shell.execute_reply":"2022-03-29T08:23:35.553695Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"token_list","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.557977Z","iopub.execute_input":"2022-03-29T08:23:35.558355Z","iopub.status.idle":"2022-03-29T08:23:35.573893Z","shell.execute_reply.started":"2022-03-29T08:23:35.558315Z","shell.execute_reply":"2022-03-29T08:23:35.573034Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# BERT Parameters\nmaxlen = 30\nbatch_size = 6\nmax_pred = 5 # max tokens of prediction\nn_layers = 6\nn_heads = 12\nd_model = 768\nd_ff = 768*4 # 4*d_model, FeedForward dimension\nd_k = d_v = 64  # dimension of K(=Q), V\nn_segments = 2","metadata":{"id":"-AuXO3rQJIUj","execution":{"iopub.status.busy":"2022-03-29T08:23:35.576188Z","iopub.execute_input":"2022-03-29T08:23:35.578279Z","iopub.status.idle":"2022-03-29T08:23:35.583287Z","shell.execute_reply.started":"2022-03-29T08:23:35.578251Z","shell.execute_reply":"2022-03-29T08:23:35.582614Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"single_sentence_len = maxlen // 2\ntokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\ntokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n\npad_a = [word2idx['[PAD]'] for i in range(single_sentence_len - len(tokens_a) - 2)] \ninput_ids_a = [word2idx['[CLS]']] + tokens_a + pad_a + [word2idx['[SEP]']]\nprint(input_ids_a)\n\npad_b = [word2idx['[PAD]'] for i in range(single_sentence_len - len(tokens_b) - 1)] \ninput_ids_b = tokens_b + pad_b + [word2idx['[SEP]']]\nprint(input_ids_b)\n\n\n# input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\ninput_ids = input_ids_a + input_ids_b\nsegment_ids = [0] * len(input_ids_a) + [1] * len(input_ids_b)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.584527Z","iopub.execute_input":"2022-03-29T08:23:35.584870Z","iopub.status.idle":"2022-03-29T08:23:35.597143Z","shell.execute_reply.started":"2022-03-29T08:23:35.584831Z","shell.execute_reply":"2022-03-29T08:23:35.596294Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# MASK LM\nn_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\nprint(n_pred)\ncand_maked_pos = [i for i, token in enumerate(input_ids)\n                    if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\nshuffle(cand_maked_pos)\nmasked_tokens, masked_pos = [], []\nfor pos in cand_maked_pos[:n_pred]:\n    masked_pos.append(pos)\n    masked_tokens.append(input_ids[pos])\n    if random() < 0.8:  # 80%\n        input_ids[pos] = word2idx['[MASK]'] # make mask\n    elif random() > 0.9:  # 10%\n        index = randint(0, vocab_size - 1) # random index in vocabulary\n        while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n            index = randint(0, vocab_size - 1)\n        input_ids[pos] = index # replace\n\n# Zero Paddings\nn_pad = maxlen - len(input_ids)\ninput_ids.extend([0] * n_pad)\nsegment_ids.extend([0] * n_pad)\n\n# Zero Padding (100% - 15%) tokens\nif max_pred > n_pred:\n    n_pad = max_pred - n_pred\n    pad_pos_idxs = np.where(np.array(input_ids) == 0)[0].tolist()  # 获取所有PAD字符在input_ids的index\n    random_pad_pos = np.random.permutation(pad_pos_idxs)\n    masked_tokens.extend([0] * n_pad)\n    masked_pos.extend(random_pad_pos[:n_pad])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.598824Z","iopub.execute_input":"2022-03-29T08:23:35.599317Z","iopub.status.idle":"2022-03-29T08:23:35.614646Z","shell.execute_reply.started":"2022-03-29T08:23:35.599278Z","shell.execute_reply":"2022-03-29T08:23:35.613606Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print([idx2word[k] for i, k in enumerate(input_ids)])","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.616053Z","iopub.execute_input":"2022-03-29T08:23:35.616530Z","iopub.status.idle":"2022-03-29T08:23:35.626264Z","shell.execute_reply.started":"2022-03-29T08:23:35.616488Z","shell.execute_reply":"2022-03-29T08:23:35.624696Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"masked_tokens, masked_pos, idx2word[input_ids[masked_pos[-1]]]","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.627892Z","iopub.execute_input":"2022-03-29T08:23:35.628236Z","iopub.status.idle":"2022-03-29T08:23:35.640165Z","shell.execute_reply.started":"2022-03-29T08:23:35.628201Z","shell.execute_reply":"2022-03-29T08:23:35.638580Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# sample IsNext and NotNext to be same in small batch size\ndef make_data():\n    batch = []\n    positive = negative = 0\n    while positive != batch_size/2 or negative != batch_size/2:\n        single_sentence_len = maxlen // 2\n        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n\n        pad_a = [word2idx['[PAD]'] for i in range(single_sentence_len - len(tokens_a) - 2)] \n        input_ids_a = [word2idx['[CLS]']] + tokens_a + pad_a + [word2idx['[SEP]']]\n\n        pad_b = [word2idx['[PAD]'] for i in range(single_sentence_len - len(tokens_b) - 1)] \n        input_ids_b = tokens_b + pad_b + [word2idx['[SEP]']]\n\n        input_ids = input_ids_a + input_ids_b\n        segment_ids = [0] * len(input_ids_a) + [1] * len(input_ids_b)\n\n        # MASK LM\n        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n#         print(n_pred)\n        cand_maked_pos = [i for i, token in enumerate(input_ids)\n                            if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n        shuffle(cand_maked_pos)\n        masked_tokens, masked_pos = [], []\n        for pos in cand_maked_pos[:n_pred]:\n            masked_pos.append(pos)\n            masked_tokens.append(input_ids[pos])\n            if random() < 0.8:  # 80%\n                input_ids[pos] = word2idx['[MASK]'] # make mask\n            elif random() > 0.9:  # 10%\n                index = randint(0, vocab_size - 1) # random index in vocabulary\n                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n                    index = randint(0, vocab_size - 1)\n                input_ids[pos] = index # replace\n\n        # Zero Paddings\n        n_pad = maxlen - len(input_ids)\n        input_ids.extend([0] * n_pad)\n        segment_ids.extend([0] * n_pad)\n\n        # Zero Padding (100% - 15%) tokens\n        if max_pred > n_pred:\n            n_pad = max_pred - n_pred\n            pad_pos_idxs = np.where(np.array(input_ids) == 0)[0].tolist()  # 获取所有PAD字符在input_ids的index\n            random_pad_pos = np.random.permutation(pad_pos_idxs)\n            masked_tokens.extend([0] * n_pad)\n            masked_pos.extend(random_pad_pos[:n_pad])\n\n        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n            positive += 1\n        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n            negative += 1\n    return batch\n# Proprecessing Finished","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.653187Z","iopub.execute_input":"2022-03-29T08:23:35.653800Z","iopub.status.idle":"2022-03-29T08:23:35.673163Z","shell.execute_reply.started":"2022-03-29T08:23:35.653760Z","shell.execute_reply":"2022-03-29T08:23:35.672319Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"batch = make_data()\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n\nclass MyDataSet(Data.Dataset):\n    def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n        self.input_ids = input_ids\n        self.segment_ids = segment_ids\n        self.masked_tokens = masked_tokens\n        self.masked_pos = masked_pos\n        self.isNext = isNext\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n\nloader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)","metadata":{"id":"bqxycRhzia7r","execution":{"iopub.status.busy":"2022-03-29T08:23:35.676723Z","iopub.execute_input":"2022-03-29T08:23:35.676986Z","iopub.status.idle":"2022-03-29T08:23:35.695575Z","shell.execute_reply.started":"2022-03-29T08:23:35.676960Z","shell.execute_reply":"2022-03-29T08:23:35.694843Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_attn_pad_mask(seq_q, seq_k):\n    batch_size, seq_len = seq_q.size()\n    # eq(zero) is PAD token\n    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n\ndef gelu(x):\n    \"\"\"\n      Implementation of the gelu activation function.\n      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n      Also see https://arxiv.org/abs/1606.08415\n    \"\"\"\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\nclass Embedding(nn.Module):\n    def __init__(self):\n        super(Embedding, self).__init__()\n        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n        self.norm = nn.LayerNorm(d_model)\n\n    def forward(self, x, seg):\n        seq_len = x.size(1)\n        pos = torch.arange(seq_len, dtype=torch.long)\n        pos = pos.unsqueeze(0).expand_as(x)  # [seq_len] -> [batch_size, seq_len]\n        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n        return self.norm(embedding)\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self):\n        super(ScaledDotProductAttention, self).__init__()\n\n    def forward(self, Q, K, V, attn_mask):\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n        attn = nn.Softmax(dim=-1)(scores)\n        context = torch.matmul(attn, V)\n        return context\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super(MultiHeadAttention, self).__init__()\n        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n        self.W_K = nn.Linear(d_model, d_k * n_heads)\n        self.W_V = nn.Linear(d_model, d_v * n_heads)\n    def forward(self, Q, K, V, attn_mask):\n        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n        residual, batch_size = Q, Q.size(0)\n        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n\n        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n\n        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads * d_v]\n        output = nn.Linear(n_heads * d_v, d_model)(context)\n        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n\nclass PoswiseFeedForwardNet(nn.Module):\n    def __init__(self):\n        super(PoswiseFeedForwardNet, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n        return self.fc2(gelu(self.fc1(x)))\n\nclass EncoderLayer(nn.Module):\n    def __init__(self):\n        super(EncoderLayer, self).__init__()\n        self.enc_self_attn = MultiHeadAttention()\n        self.pos_ffn = PoswiseFeedForwardNet()\n\n    def forward(self, enc_inputs, enc_self_attn_mask):\n        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n        return enc_outputs\n\nclass BERT(nn.Module):\n    def __init__(self):\n        super(BERT, self).__init__()\n        self.embedding = Embedding()\n        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n        self.fc = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.Dropout(0.5),\n            nn.Tanh(),\n        )\n        self.classifier = nn.Linear(d_model, 2)\n        self.linear = nn.Linear(d_model, d_model)\n        self.activ2 = gelu\n        # fc2 is shared with embedding layer\n        embed_weight = self.embedding.tok_embed.weight\n        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n        self.fc2.weight = embed_weight\n\n    def forward(self, input_ids, segment_ids, masked_pos):\n        output = self.embedding(input_ids, segment_ids) # [bach_size, seq_len, d_model]\n        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n        for layer in self.layers:\n            # output: [batch_size, max_len, d_model]\n            output = layer(output, enc_self_attn_mask)\n        # it will be decided by first token(CLS)\n        h_pooled = self.fc(output[:, 0]) # [batch_size, d_model]\n        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n\n        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n        return logits_lm, logits_clsf\n","metadata":{"id":"6inMS744xRwh","execution":{"iopub.status.busy":"2022-03-29T08:23:35.697148Z","iopub.execute_input":"2022-03-29T08:23:35.697836Z","iopub.status.idle":"2022-03-29T08:23:35.729775Z","shell.execute_reply.started":"2022-03-29T08:23:35.697797Z","shell.execute_reply":"2022-03-29T08:23:35.728833Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = BERT()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adadelta(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2022-03-29T08:23:35.731472Z","iopub.execute_input":"2022-03-29T08:23:35.731811Z","iopub.status.idle":"2022-03-29T08:23:36.101993Z","shell.execute_reply.started":"2022-03-29T08:23:35.731768Z","shell.execute_reply":"2022-03-29T08:23:36.101079Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import datetime\nstart = datetime.datetime.now()\nfor epoch in range(1000):\n    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n        logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n        loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n        loss_lm = (loss_lm).mean()\n        loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n        loss = loss_lm + loss_clsf\n        if (epoch + 1) % 10 == 0:\n            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nend = datetime.datetime.now()\nprint (end-start)","metadata":{"id":"ShYHlLr-wA_Q","outputId":"b8320337-4424-404b-c230-98dc4ec8887d","execution":{"iopub.status.busy":"2022-03-29T08:23:36.103476Z","iopub.execute_input":"2022-03-29T08:23:36.103745Z","iopub.status.idle":"2022-03-29T08:36:41.047306Z","shell.execute_reply.started":"2022-03-29T08:23:36.103709Z","shell.execute_reply":"2022-03-29T08:36:41.045584Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Predict mask tokens ans isNext\ninput_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\nprint(text)\nprint('================================')\nprint([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n\nlogits_lm, logits_clsf = model(torch.LongTensor([input_ids]), torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\nlogits_lm = logits_lm.data.max(2)[1][0].data.numpy()\nprint('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\nprint('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n\nlogits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\nprint('isNext : ', True if isNext else False)\nprint('predict isNext : ',True if logits_clsf else False)","metadata":{"id":"VMY0ypt8wC9H","outputId":"766f6373-cd9e-410d-824d-05599711c8af","execution":{"iopub.status.busy":"2022-03-29T08:36:41.048558Z","iopub.execute_input":"2022-03-29T08:36:41.048882Z","iopub.status.idle":"2022-03-29T08:36:41.162451Z","shell.execute_reply.started":"2022-03-29T08:36:41.048844Z","shell.execute_reply":"2022-03-29T08:36:41.161769Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"I-Zxz5yh7ieD"},"execution_count":null,"outputs":[]}]}